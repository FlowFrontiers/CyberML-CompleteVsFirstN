{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c19453f7-02ce-499f-96fb-f9f4fc0974b2",
   "metadata": {},
   "source": [
    "## Produce Partial Flows based on Flow Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46830152-0f9c-41f1-afe3-31769b546906",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_DIR = \"datasets\"\n",
    "INPUT_DIR = \"PCAP/deduplicated_reordered\"\n",
    "DAY = \"wednesday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0606ba5c-99ca-4043-89fb-adeb6c0a5fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating flows with NFStream: 6.5.4a\n",
      "\n",
      "\n",
      "----- FD=5ms -----\n",
      "NFStream generated flows: 2568515\n",
      "Number of flows with duration around 0.005 seconds (±20.0%): 225783\n",
      "Flows stored as: wednesday_fd_5.csv\n",
      "\n",
      "\n",
      "----- FD=10ms -----\n",
      "NFStream generated flows: 2418505\n",
      "Number of flows with duration around 0.01 seconds (±20.0%): 226310\n",
      "Flows stored as: wednesday_fd_10.csv\n",
      "\n",
      "\n",
      "----- FD=50ms -----\n",
      "NFStream generated flows: 1982856\n",
      "Number of flows with duration around 0.05 seconds (±20.0%): 149071\n",
      "Flows stored as: wednesday_fd_50.csv\n",
      "\n",
      "\n",
      "----- FD=100ms -----\n",
      "NFStream generated flows: 1849162\n",
      "Number of flows with duration around 0.1 seconds (±20.0%): 110163\n",
      "Flows stored as: wednesday_fd_100.csv\n",
      "\n",
      "\n",
      "----- FD=150ms -----\n",
      "NFStream generated flows: 1731757\n",
      "Number of flows with duration around 0.15 seconds (±20.0%): 165271\n",
      "Flows stored as: wednesday_fd_150.csv\n",
      "\n",
      "\n",
      "----- FD=300ms -----\n",
      "NFStream generated flows: 1613325\n",
      "Number of flows with duration around 0.3 seconds (±20.0%): 58211\n",
      "Flows stored as: wednesday_fd_300.csv\n",
      "\n",
      "\n",
      "----- FD=500ms -----\n",
      "NFStream generated flows: 1564196\n",
      "Number of flows with duration around 0.5 seconds (±20.0%): 37857\n",
      "Flows stored as: wednesday_fd_500.csv\n",
      "\n",
      "\n",
      "----- FD=1000ms -----\n",
      "NFStream generated flows: 1518638\n",
      "Number of flows with duration around 1.0 seconds (±20.0%): 37464\n",
      "Flows stored as: wednesday_fd_1000.csv\n",
      "\n",
      "\n",
      "----- FD=5000ms -----\n",
      "NFStream generated flows: 1447851\n",
      "Number of flows with duration around 5.0 seconds (±20.0%): 59127\n",
      "Flows stored as: wednesday_fd_5000.csv\n",
      "\n",
      "\n",
      "----- FD=10000ms -----\n",
      "NFStream generated flows: 1385940\n",
      "Number of flows with duration around 10.0 seconds (±20.0%): 271214\n",
      "Flows stored as: wednesday_fd_10000.csv\n",
      "\n",
      "\n",
      "----- FD=15000ms -----\n",
      "NFStream generated flows: 1256669\n",
      "Number of flows with duration around 15.0 seconds (±20.0%): 35406\n",
      "Flows stored as: wednesday_fd_15000.csv\n",
      "\n",
      "\n",
      "----- FD=20000ms -----\n",
      "NFStream generated flows: 1240565\n",
      "Number of flows with duration around 20.0 seconds (±20.0%): 135598\n",
      "Flows stored as: wednesday_fd_20000.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import logging\n",
    "from nfstream import NFPlugin, NFStreamer\n",
    "import nfstream\n",
    "# from labeller import cicids2017\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# set up logging\n",
    "def setup_logging(log_filename=\"generate-n-fd-flows.log\"):\n",
    "    with open(log_filename, \"w\"):  # Use 'w' to clear the existing log file, if it exists\n",
    "        pass  # Do nothing, just open and close to clear the file\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(message)s\",\n",
    "        handlers=[logging.FileHandler(log_filename), logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "def consistent_hash(value):\n",
    "    # This function converts a value into a consistent hash.\n",
    "    return hashlib.sha256(value.encode()).hexdigest()\n",
    "    \n",
    "\n",
    "class PayloadManager(NFPlugin):\n",
    "    \"\"\"Manages the payload data for network flows.\"\"\"\n",
    "\n",
    "    def on_init(self, packet, flow):\n",
    "        # Initialize payload sizes based on the packet direction.\n",
    "        flow.udps.src2dst_payload = packet.payload_size if packet.direction == 0 else 0\n",
    "        flow.udps.dst2src_payload = packet.payload_size if packet.direction == 1 else 0\n",
    "\n",
    "    def on_update(self, packet, flow):\n",
    "        # Update payload sizes based on the packet direction.\n",
    "        flow.udps.src2dst_payload += packet.payload_size if packet.direction == 0 else 0\n",
    "        flow.udps.dst2src_payload += packet.payload_size if packet.direction == 1 else 0\n",
    "\n",
    "\n",
    "class FlowExpirationManager(NFPlugin):\n",
    "    \"\"\"Manages the expiration policy for TCP flows.\"\"\"\n",
    "\n",
    "    def on_init(self, packet, flow):\n",
    "        # Set the expiration ID based on TCP rst or fin flags.\n",
    "        if packet.rst or packet.fin:\n",
    "            flow.expiration_id = -1\n",
    "\n",
    "    def on_update(self, packet, flow):\n",
    "        # Update expiration policy based on TCP rst or fin flags.\n",
    "        if packet.rst or packet.fin:\n",
    "            flow.expiration_id = -1\n",
    "\n",
    "\n",
    "class FlowLabelManager(NFPlugin):\n",
    "    \"\"\"Labels flows upon expiration.\"\"\"\n",
    "\n",
    "    def __init__(self, day):\n",
    "        self.day = day\n",
    "\n",
    "    def on_expire(self, flow):\n",
    "        # Assign a label to the flow and clean up payloads.\n",
    "        flow.udps.label = cicids2017(\n",
    "            self.day, flow, label_reverse=True, signal_reverse=False\n",
    "        )\n",
    "        self.cleanup_payload(flow)\n",
    "\n",
    "    @staticmethod\n",
    "    def cleanup_payload(flow):\n",
    "        # Clean up payload data from the flow.\n",
    "        if hasattr(flow.udps, \"src2dst_payload\"):\n",
    "            del flow.udps.src2dst_payload\n",
    "        if hasattr(flow.udps, \"dst2src_payload\"):\n",
    "            del flow.udps.dst2src_payload\n",
    "\n",
    "\n",
    "class PacketCountManager(NFPlugin):\n",
    "    \"\"\"Expire flows on specific packet count.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_packets):\n",
    "        self.max_packets = max_packets\n",
    "\n",
    "    def on_update(self, packet, flow):\n",
    "        # Check for expiration\n",
    "        if flow.bidirectional_packets == self.max_packets:\n",
    "            flow.expiration_id = -1  # Mark for expiration in NFStream\n",
    "\n",
    "class FlowDurationManager(NFPlugin):\n",
    "    \"\"\"Expire flows after a specific duration.\"\"\"\n",
    "\n",
    "    def __init__(self, max_duration_ms):\n",
    "        self.max_duration_ms = max_duration_ms\n",
    "\n",
    "    def on_update(self, packet, flow):        \n",
    "        # Check for flow expiration based on duration\n",
    "        if flow.bidirectional_duration_ms >= self.max_duration_ms:\n",
    "            flow.expiration_id = -1  # Mark for expiration in NFStream\n",
    "\n",
    "class HashManager(NFPlugin):\n",
    "    \"\"\"Calculate forward and backward hashes.\"\"\"\n",
    "    \n",
    "    def on_init(self, packet, flow):\n",
    "        # Initialize packet count and compute initial hashes\n",
    "        flow.udps.flow_key_hash = consistent_hash(f\"{packet.src_ip}-{packet.src_port}-{packet.dst_ip}-{packet.dst_port}-{packet.protocol}-{flow.bidirectional_first_seen_ms}\")\n",
    "        \n",
    "\n",
    "def process_files_in_directory(input_dir: str, day: str, output_dir: str, Ns: list):\n",
    "    \"\"\"Process all PCAP files in a directory and output to another directory.\"\"\"\n",
    "\n",
    "    BPF = \"ip and (ip proto \\\\tcp or \\\\udp)\"  # only ipv4 tcp and udp traffic to capture\n",
    "\n",
    "    for n in Ns:\n",
    "        input_file = os.path.join(input_dir, f\"rd{day.capitalize()}.pcap\")\n",
    "        if os.path.isfile(input_file):\n",
    "            output_file = os.path.join(output_dir, f\"{day}_fd_{n}.csv\")\n",
    "\n",
    "            logging.info(f\"----- FD={n}ms -----\")\n",
    "            # logging.info(f\"Processing {input_file} into {output_file}\")\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            streamer = NFStreamer(\n",
    "                  source=input_file\n",
    "                , decode_tunnels=False                                # Default: True\n",
    "                , bpf_filter=BPF                                      # Default: None\n",
    "                , promiscuous_mode=True                               # Default: True\n",
    "                , snapshot_length=1536                                # Default: 1536\n",
    "                , idle_timeout=60                                     # Default: 120\n",
    "                , active_timeout=18000                                # Default: 1800\n",
    "                , accounting_mode=1                                   # Default: 0\n",
    "                , udps=[                                              # Default: None\n",
    "                    FlowExpirationManager(),\n",
    "                    # PayloadManager(),\n",
    "                    # FlowLabelManager(day.capitalize()),\n",
    "                    HashManager(),\n",
    "                    # PacketCountManager(n),\n",
    "                    FlowDurationManager(n)\n",
    "                ]      \n",
    "                , n_dissections=0                                     # Default: 20\n",
    "                , statistical_analysis=True                           # Default: False\n",
    "                , splt_analysis=20                                     # Default: 0\n",
    "                , n_meters=1                                          # Default: 0\n",
    "                , performance_report=0                                # Default: 0\n",
    "            )\n",
    "\n",
    "            # Convert the stream to a DataFrame\n",
    "            df = streamer.to_pandas(columns_to_anonymize=[])\n",
    "            logging.info(f\"NFStream generated flows: {len(df)}\")\n",
    "\n",
    "            end = time.time()\n",
    "            processing_time = end - start\n",
    "            delta = timedelta(seconds=processing_time)\n",
    "            # logging.info(f\"Time required to generate flows: {str(delta)}\")\n",
    "\n",
    "            # Define parameters for filtering\n",
    "            max_duration_ms = n\n",
    "            tolerance = 0.20\n",
    "\n",
    "            # Keep rows where 'bidirectional_duration' is within the defined range\n",
    "            df_filtered = df[(df['bidirectional_duration_ms'] >= (max_duration_ms * (1 - tolerance))) & \n",
    "                             (df['bidirectional_duration_ms'] <= (max_duration_ms * (1 + tolerance)))]\n",
    "            logging.info(f\"Number of flows with duration around {max_duration_ms/1000} seconds (±{tolerance*100}%): {len(df_filtered)}\")\n",
    "            df = df_filtered\n",
    "\n",
    "            # Save the filtered DataFrame to a CSV file\n",
    "            df.rename(columns={\n",
    "                               # \"udps.label\": \"label\",\n",
    "                               \"udps.flow_key_hash\": \"flow_key_hash\"\n",
    "                               }, inplace=True)\n",
    "            df.to_csv(output_file, index=False)\n",
    "\n",
    "            logging.info(f\"Flows stored as: {day}_fd_{n}.csv\")\n",
    "            logging.info(f\"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = INPUT_DIR\n",
    "    output_dir = CSV_DIR\n",
    "    day = DAY\n",
    "    Ns = [5, 10, 50, 100, 150, 300, 500, 1000, 5000, 10000, 15000, 20000]\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    setup_logging()\n",
    "\n",
    "    logging.info(f\"Generating flows with NFStream: {nfstream.__version__}\")\n",
    "    logging.info(f\"\\n\")\n",
    "\n",
    "    process_files_in_directory(input_dir, day, output_dir, Ns)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d02f5f9c-8139-428c-b3b1-aa8965a37969",
   "metadata": {},
   "source": [
    "# Define the parameters for different values of n\n",
    "n_values = [5, 10, 50, 100, 150, 300, 500, 1000, 5000, 10000, 15000, 20000]\n",
    "tolerance = 0.20\n",
    "\n",
    "# Calculate the ranges for each value of n\n",
    "ranges = {}\n",
    "for n in n_values:\n",
    "    min_duration_ms = n * (1 - tolerance)\n",
    "    max_duration_ms = n * (1 + tolerance)\n",
    "    ranges[n] = (min_duration_ms, max_duration_ms)\n",
    "\n",
    "ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce430580-7323-4e9d-a80a-a3169ed9cb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
