{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f71464-f89a-49e8-98cc-a21bcc5d1c2a",
   "metadata": {},
   "source": [
    "## Prepare N Duration Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97742dc-e3a0-4496-ae89-30cd9ed13354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Directory where the CSV files are stored\n",
    "CSV_DIR = 'datasets'\n",
    "\n",
    "# List of different N values\n",
    "Ns = [5, 10, 50, 100, 150, 300, 500, 1000, 5000, 10000, 15000, 20000]\n",
    "\n",
    "tolerance = 0.10  # Define the tolerance\n",
    "\n",
    "# Load and preprocess the data for common dataframe df1\n",
    "df1 = pd.read_parquet(os.path.join(CSV_DIR, 'wednesday_cf.parquet'))\n",
    "\n",
    "# Preprocessing df1 to facilitate filtering based on 'bidirectional_packets' for different values of n\n",
    "df1 = df1[['flow_key_hash', 'bidirectional_duration_ms', 'label']]\n",
    "\n",
    "for n in Ns:\n",
    "    # Load data for each df2 based on n value\n",
    "    df2 = pd.read_csv(os.path.join(CSV_DIR, f'wednesday_fd_{n}.csv'))\n",
    "\n",
    "    # Filter df2: remove duplicate 'forward_hash' entries, keeping only the row with the lowest 'id'\n",
    "    df2.sort_values(by='id', inplace=True)\n",
    "    df2 = df2.drop_duplicates(subset='flow_key_hash', keep='first')\n",
    "\n",
    "    # Merge df1 and df2. This adds 'label' and 'bidirectional_packets' from df1 to df2\n",
    "    merged_df = df2.merge(df1, on='flow_key_hash', how='left', suffixes=('', '_df1'))\n",
    "\n",
    "    # Filter out rows where bidirectional_duration_ms from df1 is less than n and keep rows with labels\n",
    "    filtered_df = merged_df[(merged_df['bidirectional_duration_ms_df1'] >= (n * (1 - tolerance))) & merged_df['label'].notna()]\n",
    "\n",
    "    # Filter based on the new criteria for 'bidirectional_duration_ms'\n",
    "    # final_df = merged_df[((merged_df['bidirectional_duration_ms_df1'] >= (n * (1 - tolerance))) & \n",
    "    #                       (merged_df['bidirectional_duration_ms_df1'] <= (n * (1 + tolerance)))) & \n",
    "    #                      merged_df['label'].notna()]\n",
    "\n",
    "    # Drop rows where a certain label occurs fewer than 100 times\n",
    "    label_counts = filtered_df['label'].value_counts()\n",
    "    labels_to_keep = label_counts[label_counts >= 100].index\n",
    "    final_df = filtered_df[filtered_df['label'].isin(labels_to_keep)]\n",
    "    \n",
    "    # Ensure each 'flow_key_hash' is unique, then drop unnecessary columns including 'flow_key_hash'\n",
    "    final_df = final_df.drop_duplicates(subset='flow_key_hash', keep='first').drop(columns=['bidirectional_duration_ms_df1'])\n",
    "\n",
    "    # Downcast integers and floats\n",
    "    for col in final_df.columns:\n",
    "        col_type = final_df[col].dtype\n",
    "    \n",
    "        if np.issubdtype(col_type, np.integer):\n",
    "            final_df[col] = pd.to_numeric(final_df[col], downcast='integer')\n",
    "        elif np.issubdtype(col_type, np.floating):\n",
    "            final_df[col] = pd.to_numeric(final_df[col], downcast='float')    \n",
    "\n",
    "    # Store the updated dataframe in a CSV file, keeping only those flows that have a label assigned\n",
    "    final_df.to_parquet(os.path.join(CSV_DIR, f'wednesday_fd_{n}.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bd95939-9e0f-4c8b-991f-695ec5570f74",
   "metadata": {},
   "source": [
    "cf_df = pd.read_csv(os.path.join(CSV_DIR, 'wednesday_cf.csv'))\n",
    "\n",
    "# Downcast integers and floats\n",
    "for col in final_df.columns:\n",
    "    col_type = cf_df[col].dtype\n",
    "\n",
    "    if np.issubdtype(col_type, np.integer):\n",
    "        cf_df[col] = pd.to_numeric(cf_df[col], downcast='integer')\n",
    "    elif np.issubdtype(col_type, np.floating):\n",
    "        cf_df[col] = pd.to_numeric(cf_df[col], downcast='float')    \n",
    "\n",
    "cf_df.to_parquet(os.path.join(CSV_DIR, 'wednesday_cf.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e816fd3-04ca-41fa-80c9-211fde4e2316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     DS         TOTAL        BENIGN      ANOMALY        Anomaly breakdown      Min Pkts.   Mean Pkts.   Max Pkts.   \n",
      "------------ ------------ ------------ ------------ -------------------------- ---------- ------------ ------------ \n",
      "wednesday_cf    502350       326363       175987                                                                    \n",
      "                                                    DoS GoldenEye         7917          2        12.14           30 \n",
      "                                                    DoS Hulk            158680          2         9.20           27 \n",
      "                                                    DoS Slowhttptest      3707          1         4.72           35 \n",
      "                                                    DoS Slowloris         5683          2         7.36           27 \n",
      "------------ ------------ ------------ ------------ -------------------------- ---------- ------------ ------------ \n",
      "wednesday_5      7693         729          6964                                                                     \n",
      "                                                    DoS GoldenEye          270          3         7.11           15 \n",
      "                                                    DoS Hulk              5917          3         4.03           10 \n",
      "                                                    DoS Slowhttptest       777          2         3.98            4 \n",
      "------------ ------------ ------------ ------------ -------------------------- ---------- ------------ ------------ \n",
      "wednesday_10    82521         1093        81428                                                                     \n",
      "                                                    DoS GoldenEye          107          3         9.22           14 \n",
      "                                                    DoS Hulk             81321          2         3.16           11 \n",
      "------------ ------------ ------------ ------------ -------------------------- ---------- ------------ ------------ \n",
      "wednesday_50    53984        48309         5675                                                                     \n",
      "                                                    DoS Hulk              5675          3         7.37           11 \n",
      "------------ ------------ ------------ ------------ -------------------------- ---------- ------------ ------------ \n",
      "wednesday_100    46776        42783         3993                                                                     \n",
      "                                                    DoS Hulk              3993          4         7.27           11 \n",
      "------------ ------------ ------------ ------------ -------------------------- ---------- ------------ ------------ \n",
      "wednesday_150    104191       37334        66857                                                                     \n",
      "                                                    DoS Hulk             66738          6         7.81           11 \n",
      "                                                    DoS Slowhttptest       119          2         2.44            4 \n",
      "------------ ------------ ------------ ------------ -------------------------- ---------- ------------ ------------ \n",
      "wednesday_300    31784        25597         6187                                                                     \n",
      "                                                    DoS Hulk              6187          5         7.16           12 \n",
      "------------ ------------ ------------ ------------ -------------------------- ---------- ------------ ------------ \n",
      "wednesday_500    20237        17266         2971                                                                     \n",
      "                                                    DoS Hulk              2832          8         8.18           13 \n",
      "                                                    DoS Slowloris          139          5         6.02            7 \n",
      "------------ ------------ ------------ ------------ -------------------------- ---------- ------------ ------------ \n",
      "wednesday_1000    24545         9606        14939                                                                     \n",
      "                                                    DoS GoldenEye          442          2         4.08            8 \n",
      "                                                    DoS Hulk             13856          2         5.56           14 \n",
      "                                                    DoS Slowhttptest       152          2         3.16            8 \n",
      "                                                    DoS Slowloris          489          2         4.22            7 \n",
      "------------ ------------ ------------ ------------ -------------------------- ---------- ------------ ------------ \n",
      "wednesday_5000    15953        13714         2239                                                                     \n",
      "                                                    DoS GoldenEye         2239          6         8.37           18 \n",
      "------------ ------------ ------------ ------------ -------------------------- ---------- ------------ ------------ \n",
      "wednesday_10000    37060        31689         5371                                                                     \n",
      "                                                    DoS GoldenEye         5371          6        12.10           24 \n",
      "------------ ------------ ------------ ------------ -------------------------- ---------- ------------ ------------ \n",
      "wednesday_15000     8763         4243         4520                                                                     \n",
      "                                                    DoS GoldenEye          510          2        13.11           27 \n",
      "                                                    DoS Hulk              3314          2         2.06           19 \n",
      "                                                    DoS Slowhttptest       138          5         6.43           13 \n",
      "                                                    DoS Slowloris          558          6        12.68           25 \n",
      "------------ ------------ ------------ ------------ -------------------------- ---------- ------------ ------------ \n",
      "wednesday_20000    30545        29864         681                                                                      \n",
      "                                                    DoS GoldenEye          341          8        14.11           24 \n",
      "                                                    DoS Slowhttptest       340          3         5.01           19 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "\n",
    "DAY = \"wednesday\"\n",
    "\n",
    "# Initialize an empty list to store data dictionaries\n",
    "data_list = []\n",
    "\n",
    "# Initializing the table with updated formatting to match the expanded header.\n",
    "header = [\"DS\", \"TOTAL\", \"BENIGN\", \"ANOMALY\", \"Anomaly breakdown\", \"Min Pkts.\", \"Mean Pkts.\", \"Max Pkts.\"]\n",
    "# rowh = \"{:^15} \" + \"{:^7} \" + \"{:^7} \" + \"{:^7} \" + \"{:^26} \"            + \"{:^10} \" + \"{:^10} \" + \"{:^10} \"\n",
    "# row  = \"{:^15} \" + \"{:^7} \" + \"{:^7} \" + \"{:^7} \" + \"{:<18}  \" + \"{:>6} \" + \"{:>10} \" + \"{:>10} \" + \"{:>10} \"\n",
    "# sep  = [\"-\"*15]  + [\"-\"*7] + [\"-\"*7] + [\"-\"*7] + [\"-\"*26]             + [\"-\"*10]  + [\"-\"*10] + [\"-\"*10]\n",
    "rowh = \"{:^12} \" * 4 + \"{:^26} \"            + \"{:^10} \" + \"{:^12} \" + \"{:^12} \"\n",
    "row  = \"{:^12} \" * 4 + \"{:<18}  \" + \"{:>6} \" + \"{:>10} \" + \"{:>12} \" + \"{:>12} \"\n",
    "sep  = [\"-\"*12]  * 4 + [\"-\"*26]             + [\"-\"*10]  + [\"-\"*12] + [\"-\"*12]\n",
    "\n",
    "print(rowh.format(*header))  # This should now work without an IndexError\n",
    "\n",
    "Ns = ['cf'] + [5, 10, 50, 100, 150, 300, 500, 1000, 5000, 10000, 15000, 20000]\n",
    "\n",
    "for n in Ns:\n",
    "    print(rowh.format(*sep))\n",
    "\n",
    "    # Determine the filename based on the value of n\n",
    "    filename = f\"{DAY}_cf.parquet\" if n == 'cf' else f\"{DAY}_fd_{n}.parquet\"\n",
    "\n",
    "    # Load the CSV\n",
    "    csv = pd.read_parquet(os.path.join(CSV_DIR,filename))\n",
    "\n",
    "    TOTAL = len(csv)\n",
    "    BENIGN = len(csv[csv[\"label\"] == \"BENIGN\"])\n",
    "    ANOMALY = len(csv[(csv[\"label\"] != \"BENIGN\")])\n",
    "\n",
    "    # Create and append the summary row for each file\n",
    "    summary_row = {\n",
    "        \"DS\": DAY + '_' + str(n),\n",
    "        \"TOTAL\": TOTAL,\n",
    "        \"BENIGN\": BENIGN,\n",
    "        \"ANOMALY\": ANOMALY,\n",
    "        \"Anomaly breakdown\": [],\n",
    "    }\n",
    "    \n",
    "    print(row.format(DAY+'_'+str(n), TOTAL, BENIGN, ANOMALY, \"\", \"\", \"\", \"\", \"\")) \n",
    "    for label in sorted(csv[\"label\"].unique().tolist()):\n",
    "        if label in [\"BENIGN\"]:  # Skip benign and NaN labels\n",
    "            continue\n",
    "        # Calculate min, mean, and max packets for each anomaly\n",
    "        anomaly_data = csv[csv[\"label\"] == label]['bidirectional_packets']\n",
    "        # min = np.min(anomaly_data)\n",
    "        # mean = np.mean(anomaly_data)\n",
    "        # max = np.max(anomaly_data)\n",
    "        if not anomaly_data.empty:\n",
    "            min = int(np.min(anomaly_data))  # Convert to int\n",
    "            mean = float(np.mean(anomaly_data))  # Convert to float\n",
    "            max = int(np.max(anomaly_data))  # Convert to int\n",
    "        \n",
    "        print(row.format(\"\", \"\", \"\", \"\", label, len(csv[csv[\"label\"] == label]), min, \"{:.2f}\".format(mean), max))\n",
    "        \n",
    "        # Append the anomaly details to the \"Anomaly breakdown\" list\n",
    "        summary_row[\"Anomaly breakdown\"].append({\n",
    "            \"Anomaly Type\": label,\n",
    "            \"Count\": len(anomaly_data),\n",
    "            \"Min Packets\": min,\n",
    "            \"Mean Packets\": mean,\n",
    "            \"Max Packets\": max,\n",
    "        })\n",
    "    \n",
    "    data_list.append(summary_row)\n",
    "\n",
    "# Now, `data_list` contains all the information\n",
    "# Convert the list to a JSON string and write it to a file\n",
    "json_data = json.dumps(data_list, indent=4)\n",
    "with open('results/fd_anomaly_distribution.json', 'w') as file:\n",
    "    file.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc23cf7-c824-4e95-a129-23a06f4cae80",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2333e07-48db-4305-9792-26f99b0e6753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flows with label 'BENIGN' and 'bidirectional_duration_ms' >= 10: 217922\n",
      "Number of flows with label 'DoS GoldenEye' and 'bidirectional_duration_ms' >= 10: 7568\n",
      "Number of flows with label 'DoS Hulk' and 'bidirectional_duration_ms' >= 10: 144510\n",
      "Number of flows with label 'DoS Slowhttptest' and 'bidirectional_duration_ms' >= 10: 1492\n",
      "Number of flows with label 'DoS Slowloris' and 'bidirectional_duration_ms' >= 10: 2325\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X = 10\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_parquet('datasets/wednesday_cf.parquet')\n",
    "\n",
    "# Check how many rows have a certain label and 'bidirectional_duration_ms' higher than or equal to X\n",
    "benign_flows = df[(df['label'] == 'BENIGN') & (df['bidirectional_duration_ms'] >= X)].shape[0]\n",
    "goldeneye_flows = df[(df['label'] == 'DoS GoldenEye') & (df['bidirectional_duration_ms'] >= X)].shape[0]\n",
    "hulk_flows = df[(df['label'] == 'DoS Hulk') & (df['bidirectional_duration_ms'] >= X)].shape[0]\n",
    "slowhttptest_flows = df[(df['label'] == 'DoS Slowhttptest') & (df['bidirectional_duration_ms'] >= X)].shape[0]\n",
    "slowloris_flows = df[(df['label'] == 'DoS Slowloris') & (df['bidirectional_duration_ms'] >= X)].shape[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of flows with label 'BENIGN' and 'bidirectional_duration_ms' >= {X}: {benign_flows}\")\n",
    "print(f\"Number of flows with label 'DoS GoldenEye' and 'bidirectional_duration_ms' >= {X}: {goldeneye_flows}\")\n",
    "print(f\"Number of flows with label 'DoS Hulk' and 'bidirectional_duration_ms' >= {X}: {hulk_flows}\")\n",
    "print(f\"Number of flows with label 'DoS Slowhttptest' and 'bidirectional_duration_ms' >= {X}: {slowhttptest_flows}\")\n",
    "print(f\"Number of flows with label 'DoS Slowloris' and 'bidirectional_duration_ms' >= {X}: {slowloris_flows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910ee13-e3bb-43de-b9ce-da1e926a7424",
   "metadata": {},
   "source": [
    "## Convert the values into LaTeX table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a219cbe-28de-4ae6-96f1-e0998a76916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[htbp]\n",
      "\\scriptsize\n",
      "\\centering\n",
      "\\caption{Your Table Caption}\n",
      "\\renewcommand{\\arraystretch}{0.6} % Reduce spacing\n",
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "\\textbf{DS} & \\textbf{TOTAL} & \\textbf{BENIGN} & \\textbf{ANOMALY} & \\textbf{Anomaly Type} & \\textbf{Count} & \\textbf{Min Pckts.} & \\textbf{Mean Pckts.} & \\textbf{Max Pckts.} \\\\\n",
      "\\midrule\n",
      "\\multirow{4}{*}{CF} & \\multirow{4}{*}{502 350} & \\multirow{4}{*}{326 363} & \\multirow{4}{*}{175 987} & DoS GoldenEye & 7 917 & 2 & 12.14 & 30 \\\\\n",
      " & & & & DoS Hulk & 158 680 & 2 & 9.20 & 27 \\\\\n",
      " & & & & DoS Slowhttptest & 3 707 & 1 & 4.72 & 35 \\\\\n",
      " & & & & DoS Slowloris & 5 683 & 2 & 7.36 & 27 \\\\\n",
      "\\midrule\n",
      "\n",
      "\\multirow{3}{*}{FD=5ms} & \\multirow{3}{*}{7 693} & \\multirow{3}{*}{729} & \\multirow{3}{*}{6 964} & DoS GoldenEye & 270 & 3 & 7.11 & 15 \\\\\n",
      " & & & & DoS Hulk & 5 917 & 3 & 4.03 & 10 \\\\\n",
      " & & & & DoS Slowhttptest & 777 & 2 & 3.98 & 4 \\\\\n",
      "\\midrule\n",
      "\n",
      "\\multirow{2}{*}{FD=10ms} & \\multirow{2}{*}{82 521} & \\multirow{2}{*}{1 093} & \\multirow{2}{*}{81 428} & DoS GoldenEye & 107 & 3 & 9.22 & 14 \\\\\n",
      " & & & & DoS Hulk & 81 321 & 2 & 3.16 & 11 \\\\\n",
      "\\midrule\n",
      "\n",
      "FD=50ms & 53 984 & 48 309 & 5 675 & DoS Hulk & 5 675 & 3 & 7.37 & 11 \\\\\n",
      "\\midrule\n",
      "\n",
      "FD=100ms & 46 776 & 42 783 & 3 993 & DoS Hulk & 3 993 & 4 & 7.27 & 11 \\\\\n",
      "\\midrule\n",
      "\n",
      "\\multirow{2}{*}{FD=150ms} & \\multirow{2}{*}{104 191} & \\multirow{2}{*}{37 334} & \\multirow{2}{*}{66 857} & DoS Hulk & 66 738 & 6 & 7.81 & 11 \\\\\n",
      " & & & & DoS Slowhttptest & 119 & 2 & 2.44 & 4 \\\\\n",
      "\\midrule\n",
      "\n",
      "FD=300ms & 31 784 & 25 597 & 6 187 & DoS Hulk & 6 187 & 5 & 7.16 & 12 \\\\\n",
      "\\midrule\n",
      "\n",
      "\\multirow{2}{*}{FD=500ms} & \\multirow{2}{*}{20 237} & \\multirow{2}{*}{17 266} & \\multirow{2}{*}{2 971} & DoS Hulk & 2 832 & 8 & 8.18 & 13 \\\\\n",
      " & & & & DoS Slowloris & 139 & 5 & 6.02 & 7 \\\\\n",
      "\\midrule\n",
      "\n",
      "\\multirow{4}{*}{FD=1000ms} & \\multirow{4}{*}{24 545} & \\multirow{4}{*}{9 606} & \\multirow{4}{*}{14 939} & DoS GoldenEye & 442 & 2 & 4.08 & 8 \\\\\n",
      " & & & & DoS Hulk & 13 856 & 2 & 5.56 & 14 \\\\\n",
      " & & & & DoS Slowhttptest & 152 & 2 & 3.16 & 8 \\\\\n",
      " & & & & DoS Slowloris & 489 & 2 & 4.22 & 7 \\\\\n",
      "\\midrule\n",
      "\n",
      "FD=5000ms & 15 953 & 13 714 & 2 239 & DoS GoldenEye & 2 239 & 6 & 8.37 & 18 \\\\\n",
      "\\midrule\n",
      "\n",
      "FD=10000ms & 37 060 & 31 689 & 5 371 & DoS GoldenEye & 5 371 & 6 & 12.10 & 24 \\\\\n",
      "\\midrule\n",
      "\n",
      "\\multirow{4}{*}{FD=15000ms} & \\multirow{4}{*}{8 763} & \\multirow{4}{*}{4 243} & \\multirow{4}{*}{4 520} & DoS GoldenEye & 510 & 2 & 13.11 & 27 \\\\\n",
      " & & & & DoS Hulk & 3 314 & 2 & 2.06 & 19 \\\\\n",
      " & & & & DoS Slowhttptest & 138 & 5 & 6.43 & 13 \\\\\n",
      " & & & & DoS Slowloris & 558 & 6 & 12.68 & 25 \\\\\n",
      "\\midrule\n",
      "\n",
      "\\multirow{2}{*}{FD=20000ms} & \\multirow{2}{*}{30 545} & \\multirow{2}{*}{29 864} & \\multirow{2}{*}{681} & DoS GoldenEye & 341 & 8 & 14.11 & 24 \\\\\n",
      " & & & & DoS Slowhttptest & 340 & 3 & 5.01 & 19 \\\\\n",
      "\\midrule\n",
      "\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to add thousand separators\n",
    "def add_thousand_sep(number):\n",
    "    if isinstance(number, float):\n",
    "        # For floating point numbers, split on decimal point\n",
    "        integer_part, decimal_part = f\"{number:.2f}\".split(\".\")\n",
    "        integer_part_with_sep = \"{:_}\".format(int(integer_part)).replace(\"_\", \" \")\n",
    "        return f\"{integer_part_with_sep}.{decimal_part}\"\n",
    "    elif isinstance(number, int):\n",
    "        # For integers, just add the separator\n",
    "        return \"{:_}\".format(number).replace(\"_\", \" \")\n",
    "    else:\n",
    "        # Return the value as it is if it's not a number\n",
    "        return number\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open('results/fd_anomaly_distribution.json', 'r') as file:\n",
    "    data_list = json.load(file)\n",
    "\n",
    "# Start the LaTeX table and define the header\n",
    "latex_code = \"\"\"\n",
    "\\\\begin{table*}[htbp]\n",
    "\\\\scriptsize\n",
    "\\\\centering\n",
    "\\\\caption{Your Table Caption}\n",
    "\\\\renewcommand{\\\\arraystretch}{0.6} % Reduce spacing\n",
    "\\\\begin{tabular}{lrrrrrrrr}\n",
    "\\\\toprule\n",
    "\\\\textbf{DS} & \\\\textbf{TOTAL} & \\\\textbf{BENIGN} & \\\\textbf{ANOMALY} & \\\\textbf{Anomaly Type} & \\\\textbf{Count} & \\\\textbf{Min Pckts.} & \\\\textbf{Mean Pckts.} & \\\\textbf{Max Pckts.} \\\\\\\\\n",
    "\\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Helper to determine how many anomaly breakdowns are present for a given DS\n",
    "def count_anomalies(entry):\n",
    "    return len(entry.get(\"Anomaly breakdown\", []))\n",
    "\n",
    "# Iterate over each entry in the data list to populate the table rows\n",
    "for entry in data_list:\n",
    "    # Extract DS value and transform it according to the specified rules\n",
    "    ds_value = entry['DS']\n",
    "    if ds_value == \"wednesday_cf\":\n",
    "        ds_label = \"CF\"\n",
    "    elif ds_value.startswith(\"wednesday_\"):\n",
    "        ds_number = ds_value.split(\"_\")[-1]  # Extract the number part\n",
    "        ds_label = f\"FD={ds_number}ms\"\n",
    "    else:\n",
    "        ds_label = ds_value  # Fallback to the original DS value if none of the above rules apply\n",
    "\n",
    "    anomaly_count = count_anomalies(entry)\n",
    "\n",
    "    # Format the numbers with thousand separator\n",
    "    total_formatted = add_thousand_sep(entry['TOTAL'])\n",
    "    benign_formatted = add_thousand_sep(entry['BENIGN'])\n",
    "    anomaly_formatted = add_thousand_sep(entry['ANOMALY'])\n",
    "\n",
    "    # Add the summary row for each dataset\n",
    "    # Check if there's more than one type of anomaly for multirow\n",
    "    anomaly_breakdown = entry['Anomaly breakdown']\n",
    "    if len(anomaly_breakdown) > 1:\n",
    "        latex_code += f\"\\\\multirow{{{len(anomaly_breakdown)}}}{{*}}{{{ds_label}}} & \"\n",
    "        latex_code += f\"\\\\multirow{{{len(anomaly_breakdown)}}}{{*}}{{{add_thousand_sep(entry['TOTAL'])}}} & \"\n",
    "        latex_code += f\"\\\\multirow{{{len(anomaly_breakdown)}}}{{*}}{{{add_thousand_sep(entry['BENIGN'])}}} & \"\n",
    "        latex_code += f\"\\\\multirow{{{len(anomaly_breakdown)}}}{{*}}{{{add_thousand_sep(entry['ANOMALY'])}}} & \"\n",
    "    else:\n",
    "        # Only one type of anomaly, no need for multirow\n",
    "        latex_code += f\"{ds_label} & {add_thousand_sep(entry['TOTAL'])} & \"\n",
    "        latex_code += f\"{add_thousand_sep(entry['BENIGN'])} & {add_thousand_sep(entry['ANOMALY'])} & \"\n",
    "\n",
    "    # Add rows for the anomaly breakdown\n",
    "    for i, anomaly in enumerate(anomaly_breakdown):\n",
    "        if i > 0:  # Not the first entry, so we need to start a new table row\n",
    "            latex_code += \" & & & & \"\n",
    "        latex_code += f\"{anomaly['Anomaly Type']} & {add_thousand_sep(anomaly['Count'])} & \"\n",
    "        latex_code += f\"{add_thousand_sep(anomaly['Min Packets'])} & {add_thousand_sep(anomaly['Mean Packets'])} & \"\n",
    "        latex_code += f\"{add_thousand_sep(anomaly['Max Packets'])} \\\\\\\\\\n\"\n",
    "\n",
    "    # Add a midrule after each DS block if there are multiple anomalies\n",
    "    latex_code += \"\\\\midrule\\n\\n\"\n",
    "\n",
    "# Close the LaTeX table structure\n",
    "latex_code += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table*}\"\n",
    "\n",
    "# Display the generated LaTeX code (for testing purposes)\n",
    "print(latex_code)\n",
    "\n",
    "with open('results/fd_anomaly_distribution_table.tex', 'w') as file:\n",
    "    file.write(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9b70c-68f3-4214-9354-8f5b56ac15b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
