{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa837a26-ee8b-4074-b2c3-df8a0002b83f",
   "metadata": {},
   "source": [
    "# Preliminary Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212796df-b8eb-49a5-a51b-df08df39f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"results\"\n",
    "CSV_DIR = \"datasets\"\n",
    "INPUT_DIR = \"PCAP/deduplicated_reordered\"\n",
    "DAY = \"wednesday\"\n",
    "\n",
    "SCENARIO = \"initial\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ddc14-a466-4ed6-9ea4-0728ead7c47b",
   "metadata": {},
   "source": [
    "## For this flow measurement \n",
    "we set\n",
    " - `idle timeout` to `60 seconds`\n",
    " - `active timeout` to `120 seconds`\n",
    "\n",
    "to reflect the measurement in the original dataset as closely as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c76fcc-cd5c-4edd-bbe8-5400392dd562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating flows with NFStream v6.5.4a\n",
      "\n",
      "\n",
      "NFStream generated flows: 504474\n",
      "Flows stored as: wednesday_initial.parquet\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import logging\n",
    "from nfstream import NFPlugin, NFStreamer\n",
    "import nfstream\n",
    "from labeller import cicids2017\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set up logging\n",
    "def setup_logging(log_filename=\"generate-initial-flows.log\"):\n",
    "    with open(log_filename, \"w\"):  # Use 'w' to clear the existing log file, if it exists\n",
    "        pass  # Do nothing, just open and close to clear the file\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(message)s\",\n",
    "        handlers=[logging.FileHandler(log_filename), logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "\n",
    "def consistent_hash(value):\n",
    "    # This function converts a value into a consistent hash.\n",
    "    return hashlib.sha256(value.encode()).hexdigest()\n",
    "    \n",
    "\n",
    "class PayloadManager(NFPlugin):\n",
    "    \"\"\"Manages the payload data for network flows.\"\"\"\n",
    "\n",
    "    def on_init(self, packet, flow):\n",
    "        # Initialize payload sizes based on the packet direction.\n",
    "        flow.udps.src2dst_payload = packet.payload_size if packet.direction == 0 else 0\n",
    "        flow.udps.dst2src_payload = packet.payload_size if packet.direction == 1 else 0\n",
    "\n",
    "    def on_update(self, packet, flow):\n",
    "        # Update payload sizes based on the packet direction.\n",
    "        flow.udps.src2dst_payload += packet.payload_size if packet.direction == 0 else 0\n",
    "        flow.udps.dst2src_payload += packet.payload_size if packet.direction == 1 else 0\n",
    "\n",
    "\n",
    "class FlowExpirationManager(NFPlugin):\n",
    "    \"\"\"Manages the expiration policy for TCP flows.\"\"\"\n",
    "\n",
    "    def on_init(self, packet, flow):\n",
    "        # Set the expiration ID based on TCP rst or fin flags.\n",
    "        if packet.rst or packet.fin:\n",
    "            flow.expiration_id = -1\n",
    "\n",
    "    def on_update(self, packet, flow):\n",
    "        # Update expiration policy based on TCP rst or fin flags.\n",
    "        if packet.rst or packet.fin:\n",
    "            flow.expiration_id = -1\n",
    "\n",
    "\n",
    "class FlowLabelManager(NFPlugin):\n",
    "    \"\"\"Labels flows upon expiration.\"\"\"\n",
    "\n",
    "    def __init__(self, day):\n",
    "        self.day = day\n",
    "\n",
    "    def on_expire(self, flow):\n",
    "        # Assign a label to the flow and clean up payloads.\n",
    "        flow.udps.label = cicids2017(\n",
    "            self.day, flow, label_reverse=True, signal_reverse=False\n",
    "        )\n",
    "        self.cleanup_payload(flow)\n",
    "\n",
    "    @staticmethod\n",
    "    def cleanup_payload(flow):\n",
    "        # Clean up payload data from the flow.\n",
    "        if hasattr(flow.udps, \"src2dst_payload\"):\n",
    "            del flow.udps.src2dst_payload\n",
    "        if hasattr(flow.udps, \"dst2src_payload\"):\n",
    "            del flow.udps.dst2src_payload\n",
    "\n",
    "\n",
    "class HashManager(NFPlugin):\n",
    "    \"\"\"Calculate forward and backward hashes.\"\"\"\n",
    "    def on_init(self, packet, flow):\n",
    "        flow.udps.flow_key_hash = consistent_hash(f\"{packet.src_ip}-{packet.src_port}-{packet.dst_ip}-{packet.dst_port}-{packet.protocol}\")\n",
    "\n",
    "            \n",
    "def process_files_in_directory(input_dir: str, day: str, output_dir: str):\n",
    "    \"\"\"Process all PCAP files in a directory and output to another directory.\"\"\"\n",
    "\n",
    "    BPF = \"ip and (ip proto \\\\tcp or \\\\udp)\"  # only ipv4 tcp and udp traffic to capture\n",
    "\n",
    "    input_file = os.path.join(input_dir, f\"rd{day.capitalize()}.pcap\")\n",
    "    if os.path.isfile(input_file):\n",
    "        output_file = os.path.join(output_dir, f\"{day}_{SCENARIO}.parquet\")\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        streamer = NFStreamer(\n",
    "              source=input_file\n",
    "            , decode_tunnels=False                                # Default: True\n",
    "            , bpf_filter=BPF                                      # Default: None\n",
    "            , promiscuous_mode=True                               # Default: True\n",
    "            , snapshot_length=1536                                # Default: 1536\n",
    "            , idle_timeout=60                                     # Default: 120\n",
    "            , active_timeout=120                                  # Default: 1800\n",
    "            , accounting_mode=1                                   # Default: 0\n",
    "            , udps=[                                              # Default: None\n",
    "                # FlowExpirationManager(),\n",
    "                PayloadManager(),\n",
    "                HashManager(),\n",
    "                FlowLabelManager(day.capitalize())\n",
    "            ]      \n",
    "            , n_dissections=0                                     # Default: 20\n",
    "            , statistical_analysis=True                           # Default: False\n",
    "            , splt_analysis=20                                    # Default: 0\n",
    "            , n_meters=1                                          # Default: 0\n",
    "            , performance_report=0                                # Default: 0\n",
    "        )\n",
    "\n",
    "        # Convert the stream to a DataFrame\n",
    "        df = streamer.to_pandas(columns_to_anonymize=[])\n",
    "        logging.info(f\"NFStream generated flows: {len(df)}\")\n",
    "\n",
    "        end = time.time()\n",
    "        processing_time = end - start\n",
    "        delta = timedelta(seconds=processing_time)\n",
    "        # logging.info(f\"Time required to generate flows: {str(delta)}\")\n",
    "\n",
    "        # Rename column titles\n",
    "        df.rename(columns={\n",
    "                           \"udps.label\": \"label\",\n",
    "                           \"udps.flow_key_hash\": \"flow_key_hash\"\n",
    "                          }, inplace=True)\n",
    "        \n",
    "        # Downcast integers and floats\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtype\n",
    "        \n",
    "            if np.issubdtype(col_type, np.integer):\n",
    "                df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "            elif np.issubdtype(col_type, np.floating):\n",
    "                df[col] = pd.to_numeric(df[col], downcast='float') \n",
    "        \n",
    "        # Save the filtered DataFrame to a parquet file\n",
    "        df.to_parquet(output_file, index=False)\n",
    "\n",
    "        logging.info(f\"Flows stored as: {day}_{SCENARIO}.parquet\")\n",
    "        logging.info(f\"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = INPUT_DIR\n",
    "    output_dir = CSV_DIR\n",
    "    day = DAY\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    setup_logging()\n",
    "\n",
    "    logging.info(f\"Generating flows with NFStream v{nfstream.__version__}\")\n",
    "    logging.info(f\"\\n\")\n",
    "    \n",
    "    process_files_in_directory(input_dir, day, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3991051-44ce-439e-9e1e-dc2efbd671d2",
   "metadata": {},
   "source": [
    "## Show dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66789c4-7b97-458b-b079-773c12ec816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DS        TOTAL       BENIGN    BENIGN - ZPL   ANOMALY    ANOMALY - ZPL         Anomaly breakdown        \n",
      "----------  ----------  ----------  ------------  ----------  ------------  --------------------------------\n",
      "wednesday     504474      310381       13580        174156        6357                                    \n",
      "                                                                            DoS GoldenEye             7916\n",
      "                                                                            DoS GoldenEye - ZPL        870\n",
      "                                                                            DoS Hulk                158027\n",
      "                                                                            DoS Hulk - ZPL             594\n",
      "                                                                            DoS Slowhttptest          3010\n",
      "                                                                            DoS Slowhttptest - ZPL    3088\n",
      "                                                                            DoS Slowloris             5192\n",
      "                                                                            DoS Slowloris - ZPL       1805\n",
      "                                                                            Heartbleed                  11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "day = DAY\n",
    "\n",
    "# Initializing the table.\n",
    "header = [\"DS\", \"TOTAL\", \"BENIGN\", \"BENIGN - ZPL\", \"ANOMALY\", \"ANOMALY - ZPL\", \"Anomaly breakdown\"]\n",
    "rowh = \"{:^10}  \" + \"{:^10}  \" + \"{:^10}  \" + \"{:^12}  \" + \"{:^10}  \" + \"{:^12}  \" + \"{:^32}\"\n",
    "row  = \"{:^10}  \" + \"{:^10}  \" + \"{:^10}  \" + \"{:^12}  \" + \"{:^10}  \" + \"{:^12}  \" + \"{:<24}\" + \"{:>6}\"\n",
    "sep  = [\"-\"*10] + [\"-\"*10] + [\"-\"*10] + [\"-\"*12] + [\"-\"*10] + [\"-\"*12] + [\"-\"*32]\n",
    "print(rowh.format(*header))\n",
    "print(rowh.format(*sep))\n",
    "\n",
    "csv = pd.read_parquet(os.path.join(CSV_DIR,f\"{day}_{SCENARIO}.parquet\"))\n",
    "\n",
    "TOTAL = len(csv)\n",
    "BENIGN = len(csv[csv[\"label\"] == \"BENIGN\"])\n",
    "BENIGNZPL = len(csv[csv[\"label\"] == \"BENIGN - ZPL\"])\n",
    "ANOMALY = len(csv[(csv[\"label\"] != \"BENIGN\") & (csv[\"label\"] != \"BENIGN - ZPL\") & (~csv[\"label\"].str.contains(\"ZPL\"))])\n",
    "ANOMALYZPL = len(csv[(csv[\"label\"] != \"BENIGN\") & (csv[\"label\"] != \"BENIGN - ZPL\") & (csv[\"label\"].str.contains(\"ZPL\"))])\n",
    "\n",
    "print(row.format(DAY, TOTAL, BENIGN, BENIGNZPL, ANOMALY, ANOMALYZPL, \"\", \"\"))\n",
    "for label in sorted(csv[\"label\"].unique().tolist()):\n",
    "    if label == \"BENIGN\":\n",
    "        continue\n",
    "    if label == \"BENIGN - ZPL\":\n",
    "        continue\n",
    "    print(row.format(\"\",\"\",\"\",\"\",\"\",\"\", label, len(csv[csv[\"label\"] == label])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e5d3f-ef3c-4872-9063-6723b84a8cc6",
   "metadata": {},
   "source": [
    "We observe a considerable number of flows with zero packet payload, a pattern that is not characteristic of any of the attacks in the Wednesday dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0a8a8-3730-42e0-8048-0610784792cf",
   "metadata": {},
   "source": [
    "## Show FIN and RST statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbd29a-4c2a-48f5-8d96-e387703f2d4f",
   "metadata": {},
   "source": [
    "### Count of flows whose FIN or RST count is higher than 1, 2, and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d9144c-f54c-4d79-83f9-58b7fe8f741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " T_FIN>1    B_FIN>1    A_FIN>1    T_RST>1    B_RST>1    A_RST>1   \n",
      "---------  ---------  ---------  ---------  ---------  ---------  \n",
      " 153551      81030      72521      96880      10019      86861    \n",
      "\n",
      "\n",
      " T_FIN>2    B_FIN>2    A_FIN>2    T_RST>2    B_RST>2    A_RST>2   \n",
      "---------  ---------  ---------  ---------  ---------  ---------  \n",
      "  8827       3008       5819       40832      2273       38559    \n",
      "\n",
      "\n",
      " T_FIN>3    B_FIN>3    A_FIN>3    T_RST>3    B_RST>3    A_RST>3   \n",
      "---------  ---------  ---------  ---------  ---------  ---------  \n",
      "  1106        585        521       7090       1290       5800     \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for N in range(1,1+3):\n",
    "    # Initializing the table.\n",
    "    header = [\n",
    "              f\"T_FIN>{N}\", f\"B_FIN>{N}\", f\"A_FIN>{N}\",\n",
    "              f\"T_RST>{N}\", f\"B_RST>{N}\", f\"A_RST>{N}\"]\n",
    "    rowh = \"{:^9}  \"*(len(header))\n",
    "    row = \"{:^9}  \"*(len(header))\n",
    "    sep = [\"-\"*9]*(len(header)) + [\"-\"*39]\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(rowh.format(*header)) \n",
    "    print(rowh.format(*sep))\n",
    "    \n",
    "    FIN_GT = len(csv[csv[\"bidirectional_fin_packets\"] > N])\n",
    "    RST_GT = len(csv[csv[\"bidirectional_rst_packets\"] > N])\n",
    "\n",
    "    BENIGN_FIN_GT = len(csv[(csv[\"label\"] == \"BENIGN\") & (csv[\"bidirectional_fin_packets\"] > N)])\n",
    "    BENIGN_RST_GT = len(csv[(csv[\"label\"] == \"BENIGN\") & (csv[\"bidirectional_rst_packets\"] > N)])\n",
    "\n",
    "    ANOMALY_FIN_GT = len(csv[(csv[\"label\"] != \"BENIGN\") & (csv[\"bidirectional_fin_packets\"] > N)])\n",
    "    ANOMALY_RST_GT = len(csv[(csv[\"label\"] != \"BENIGN\") & (csv[\"bidirectional_rst_packets\"] > N)])\n",
    "    \n",
    "    print(row.format(\n",
    "\n",
    "                     FIN_GT, BENIGN_FIN_GT, ANOMALY_FIN_GT,\n",
    "                     RST_GT, BENIGN_RST_GT, ANOMALY_RST_GT))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29075605-0230-4cc7-b0a6-fea6dd896095",
   "metadata": {},
   "source": [
    "## Show statistics for the FIN and RST counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ad8c1a9-e8d2-47e5-a4f4-720a3d814100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       bidirectional_fin_packets  bidirectional_rst_packets\n",
      "count              504474.000000              504474.000000\n",
      "mean                    0.856978                   0.678846\n",
      "std                     0.927540                   1.058333\n",
      "min                     0.000000                   0.000000\n",
      "25%                     0.000000                   0.000000\n",
      "50%                     1.000000                   0.000000\n",
      "75%                     2.000000                   1.000000\n",
      "max                    13.000000                 139.000000\n"
     ]
    }
   ],
   "source": [
    "print(csv[['bidirectional_fin_packets', 'bidirectional_rst_packets']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2abcf34-d014-464b-b591-7238a63776a6",
   "metadata": {},
   "source": [
    "## Show top 10 FIN and RST counts with the highest occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb8d488-86d9-4c03-ba95-38443f6e2e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for wednesday:\n",
      "\n",
      "Top 10 Value Counts for FIN Packets:\n",
      "0     237715\n",
      "2     144724\n",
      "1     113208\n",
      "3       7721\n",
      "4        377\n",
      "5        215\n",
      "6        202\n",
      "7         96\n",
      "10        84\n",
      "8         83\n",
      "\n",
      "Top 10 Value Counts for RST Packets:\n",
      "0    309024\n",
      "1     98570\n",
      "2     56048\n",
      "3     33742\n",
      "4      6117\n",
      "5       641\n",
      "6       155\n",
      "7        74\n",
      "9        32\n",
      "8        16\n"
     ]
    }
   ],
   "source": [
    "N=10\n",
    "\n",
    "# # Get descriptive statistics\n",
    "print(f\"Statistics for {day}:\")\n",
    "# stats = csv[['bidirectional_fin_packets', 'bidirectional_rst_packets']].describe(percentiles=[0.5, 0.75, 0.9, 0.95, 1.0])\n",
    "# print(stats)\n",
    "\n",
    "# Print the top N value counts for each column without the index name and series description\n",
    "fin_counts = csv['bidirectional_fin_packets'].value_counts().head(N).to_string(header=False, index=True)\n",
    "rst_counts = csv['bidirectional_rst_packets'].value_counts().head(N).to_string(header=False, index=True)\n",
    "\n",
    "print(f\"\\nTop {N} Value Counts for FIN Packets:\")\n",
    "print(fin_counts)\n",
    "\n",
    "print(f\"\\nTop {N} Value Counts for RST Packets:\")\n",
    "print(rst_counts)\n",
    "\n",
    "# print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c315144-0dfd-4550-9e79-1f888501f8fe",
   "metadata": {},
   "source": [
    "## Show flow expiration statistics\n",
    "\n",
    "expiration_id =\n",
    " - 0 for idle timeout,\n",
    " - 1 for active timeout, or\n",
    " - -1 for custom expiration.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea7593f-8fd3-4583-93a7-b36997fb1fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expiration_id\n",
      "0    493226\n",
      "1     11248\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = csv\n",
    "\n",
    "print(df['expiration_id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265937b-ae85-4f6f-8a09-e1bde406ed37",
   "metadata": {},
   "source": [
    "## Check flows associated with `Heartbleed` attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7651a4e5-2e3f-44f9-b822-93f74aae3fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        expiration_id         src_ip  src_port         dst_ip  dst_port  protocol  bidirectional_duration_ms  bidirectional_packets       label\n",
      "430915              1     172.16.0.1     45022  192.168.10.51       444         6                     119303                   4414  Heartbleed\n",
      "433835              1  192.168.10.51       444     172.16.0.1     45022         6                     119262                   4902  Heartbleed\n",
      "437445              1  192.168.10.51       444     172.16.0.1     45022         6                     119261                   4924  Heartbleed\n",
      "439999              1  192.168.10.51       444     172.16.0.1     45022         6                     119260                   4905  Heartbleed\n",
      "444467              1  192.168.10.51       444     172.16.0.1     45022         6                     119298                   4871  Heartbleed\n",
      "446675              1  192.168.10.51       444     172.16.0.1     45022         6                     119260                   4873  Heartbleed\n",
      "448944              1  192.168.10.51       444     172.16.0.1     45022         6                     119259                   4870  Heartbleed\n",
      "450000              1  192.168.10.51       444     172.16.0.1     45022         6                     119258                   4869  Heartbleed\n",
      "452161              1  192.168.10.51       444     172.16.0.1     45022         6                     119299                   4833  Heartbleed\n",
      "452861              1  192.168.10.51       444     172.16.0.1     45022         6                     119297                   4803  Heartbleed\n",
      "455225              0  192.168.10.51       444     172.16.0.1     45022         6                      25052                   1032  Heartbleed\n"
     ]
    }
   ],
   "source": [
    "print(df[df['label'] == 'Heartbleed'][['expiration_id', 'src_ip', 'src_port', 'dst_ip', 'dst_port', 'protocol', 'bidirectional_duration_ms', 'bidirectional_packets', 'label']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29e71c-78c3-42bd-b4ab-56a2d6b1e4f0",
   "metadata": {},
   "source": [
    "This attack was one long attack divided by active timeout into multiple instances. Each flow except the last was 120 seconds long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fafadc-5a58-4c76-aa76-aaed0c409bc7",
   "metadata": {},
   "source": [
    "## Show time related statistics for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29f33dc6-d927-4165-a94a-9ae58c5cb859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats about bidirectional_duration_ms:\n",
      "count    504474.000000\n",
      "mean      10310.873072\n",
      "std       28498.878528\n",
      "min           0.000000\n",
      "25%          25.000000\n",
      "50%         136.000000\n",
      "75%        1015.000000\n",
      "max      119999.000000\n",
      "\n",
      "Show top N longest flow values and their occurence:\n",
      "119999    39\n",
      "119998    10\n",
      "119997     8\n",
      "119996    13\n",
      "119995    10\n",
      "119994     6\n",
      "119993     3\n",
      "119992     6\n",
      "119991     5\n",
      "119990     6\n",
      "\n",
      "Show a sample of rows with the longest duration:\n",
      "       src_ip  src_port        dst_ip  dst_port  protocol  bidirectional_duration_ms  label\n",
      "192.168.10.14     49533   13.107.4.50        80         6                     119999 BENIGN\n",
      "  13.107.4.50        80 192.168.10.14     49533         6                     119999 BENIGN\n",
      "  13.107.4.50        80 192.168.10.14     49533         6                     119999 BENIGN\n",
      "192.168.10.15     49672   13.107.4.50        80         6                     119999 BENIGN\n",
      "  13.107.4.50        80 192.168.10.15     49672         6                     119999 BENIGN\n",
      "192.168.10.15     49672   13.107.4.50        80         6                     119999 BENIGN\n",
      "  13.107.4.50        80 192.168.10.15     49672         6                     119999 BENIGN\n",
      " 192.168.10.9     14580 74.125.29.154       443         6                     119999 BENIGN\n",
      "178.172.160.2       443 192.168.10.12     50824         6                     119999 BENIGN\n",
      "178.172.160.2       443 192.168.10.12     50824         6                     119999 BENIGN\n"
     ]
    }
   ],
   "source": [
    "print(\"Stats about bidirectional_duration_ms:\")\n",
    "print(df['bidirectional_duration_ms'].describe().to_string())\n",
    "\n",
    "print(\"\\nShow top N longest flow values and their occurence:\")\n",
    "print(df['bidirectional_duration_ms'].value_counts().sort_index(ascending=False).head(10).to_string(header=False))\n",
    "\n",
    "print(\"\\nShow a sample of rows with the longest duration:\")\n",
    "print(df[df['bidirectional_duration_ms'] == df['bidirectional_duration_ms'].max()]\n",
    "      [['src_ip', 'src_port', 'dst_ip', 'dst_port', 'protocol', 'bidirectional_duration_ms', 'label']]\n",
    "      .head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b981f3-006a-4869-a089-e285b970d304",
   "metadata": {},
   "source": [
    "## Show the number of flows whose unique ID appears more than once in the DS\n",
    "\n",
    "Such flows are esentially repeated across the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28b7aad5-747a-4025-a32a-8b6278e6dd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Label  Hash_More_Than_One\n",
      "0                   BENIGN               34903\n",
      "1             BENIGN - ZPL                  85\n",
      "2            DoS Slowloris                 278\n",
      "3      DoS Slowloris - ZPL                   0\n",
      "4         DoS Slowhttptest                1042\n",
      "5   DoS Slowhttptest - ZPL                   0\n",
      "6                 DoS Hulk               14267\n",
      "7           DoS Hulk - ZPL                   1\n",
      "8            DoS GoldenEye                 414\n",
      "9      DoS GoldenEye - ZPL                   0\n",
      "10              Heartbleed                   1\n"
     ]
    }
   ],
   "source": [
    "# Prepare an empty list to store results\n",
    "results_list = []\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = df['label'].unique()\n",
    "\n",
    "# Iterate through each unique label\n",
    "for label in unique_labels:\n",
    "    # Filter the dataset for the current label\n",
    "    df_label = df[df['label'] == label]\n",
    "    \n",
    "    # Count the occurrences of each unique value in the 'udps.forward_hash' and 'udps.backward_hash' columns\n",
    "    value_counts = df_label['flow_key_hash'].value_counts()\n",
    "\n",
    "    # print(value_counts_f.head(10))\n",
    "    \n",
    "    # Count how many unique values have more than one occurrence\n",
    "    more_than_one_unique = sum(value_counts > 1)\n",
    "    \n",
    "    # Append the results for the current label to the results list\n",
    "    results_list.append({'Label': label, \n",
    "                         'Hash_More_Than_One': more_than_one_unique\n",
    "                        })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Print the result\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5c59c-6295-4e77-9d97-71c94d8c5f5c",
   "metadata": {},
   "source": [
    "## Insights into the temporal distribution of flows for each label based on the `bidirectional_first_seen_ms` timestamp\n",
    "\n",
    "The output is a new DataFrame (`stats`) where each row corresponds to a unique label from our data, and the columns include:\n",
    "\n",
    "- `label`: The unique identifier for each group of flows.\n",
    "- `min`: The minimum time difference between consecutive flows within the same label group.\n",
    "- `max`: The maximum time difference between consecutive flows within the same label group.\n",
    "- `mean`: The average time difference between consecutive flows within the same label group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c179f77-bd64-4a0a-afd8-ec2c2096a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     label       min       max           mean\n",
      "0                   BENIGN       0.0   93789.0      98.113516\n",
      "1             BENIGN - ZPL       0.0  234860.0    2236.216953\n",
      "2            DoS GoldenEye       0.0  158048.0      76.523563\n",
      "3      DoS GoldenEye - ZPL       0.0  130868.0     270.453395\n",
      "4                 DoS Hulk       0.0   52243.0       6.905940\n",
      "5           DoS Hulk - ZPL       0.0  286686.0    2290.623946\n",
      "6         DoS Slowhttptest       0.0  174425.0     442.470588\n",
      "7   DoS Slowhttptest - ZPL       0.0  397290.0     373.832523\n",
      "8            DoS Slowloris       0.0   11879.0     252.041803\n",
      "9      DoS Slowloris - ZPL       0.0   33944.0     712.571508\n",
      "10              Heartbleed  120254.0  120297.0  120259.200000\n",
      "\n",
      "The same in a human readable form:\n",
      "\n",
      "                     label           min            max          mean\n",
      "0                   BENIGN       0.00 ms   1.56 minutes      98.11 ms\n",
      "1             BENIGN - ZPL       0.00 ms   3.91 minutes  2.24 seconds\n",
      "2            DoS GoldenEye       0.00 ms   2.63 minutes      76.52 ms\n",
      "3      DoS GoldenEye - ZPL       0.00 ms   2.18 minutes     270.45 ms\n",
      "4                 DoS Hulk       0.00 ms  52.24 seconds       6.91 ms\n",
      "5           DoS Hulk - ZPL       0.00 ms   4.78 minutes  2.29 seconds\n",
      "6         DoS Slowhttptest       0.00 ms   2.91 minutes     442.47 ms\n",
      "7   DoS Slowhttptest - ZPL       0.00 ms   6.62 minutes     373.83 ms\n",
      "8            DoS Slowloris       0.00 ms  11.88 seconds     252.04 ms\n",
      "9      DoS Slowloris - ZPL       0.00 ms  33.94 seconds     712.57 ms\n",
      "10              Heartbleed  2.00 minutes   2.00 minutes  2.00 minutes\n"
     ]
    }
   ],
   "source": [
    "# Sort the dataframe by label and then by first_seen_ms to ensure the order\n",
    "df_sorted = df.sort_values(by=['label', 'bidirectional_first_seen_ms'])\n",
    "\n",
    "# Calculate the difference in 'bidirectional_first_seen_ms' between subsequent rows within each 'label'\n",
    "df_sorted['time_diff'] = df_sorted.groupby('label')['bidirectional_first_seen_ms'].diff()\n",
    "\n",
    "# Now, group by 'label' and calculate min, max, and mean differences\n",
    "stats = df_sorted.groupby('label')['time_diff'].agg(['min', 'max', 'mean']).reset_index()\n",
    "\n",
    "# Print the resulting statistics for each label\n",
    "print(stats)\n",
    "\n",
    "print(\"\\nThe same in a human readable form:\\n\")\n",
    "\n",
    "# Convert milliseconds to more readable units\n",
    "def convert_to_readable_time(ms):\n",
    "    if pd.isna(ms):  # Check for NaN values\n",
    "        return 'N/A'  # Return 'N/A' for NaN values\n",
    "    if ms < 1000:\n",
    "        return f\"{ms:.2f} ms\"  # Keep milliseconds if less than one second\n",
    "    elif ms < 60000:\n",
    "        return f\"{ms / 1000:.2f} seconds\"  # Convert to seconds if less than one minute\n",
    "    elif ms < 3600000:\n",
    "        return f\"{ms / 60000:.2f} minutes\"  # Convert to minutes if less than one hour\n",
    "    else:\n",
    "        return f\"{ms / 3600000:.2f} hours\"  # Convert to hours otherwise\n",
    "\n",
    "# Apply the conversion to each time column\n",
    "stats['min'] = stats['min'].apply(convert_to_readable_time)\n",
    "stats['max'] = stats['max'].apply(convert_to_readable_time)\n",
    "stats['mean'] = stats['mean'].apply(convert_to_readable_time)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e50ea-df08-4d8d-a258-cb0eed21360d",
   "metadata": {},
   "source": [
    "## Analyze the flow duration in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b9afa4f-7e90-4589-874a-db5f2cc73ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    504474.000000\n",
      "mean      10310.873072\n",
      "std       28498.878528\n",
      "min           0.000000\n",
      "25%          25.000000\n",
      "50%         136.000000\n",
      "75%        1015.000000\n",
      "max      119999.000000\n",
      "Name: bidirectional_duration_ms, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['bidirectional_duration_ms'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a58d97-dc47-46e2-a05e-c8a106ec4b77",
   "metadata": {},
   "source": [
    "By looking at these statistics, we can get a good overall picture of the distribution and scale of the bidirectional durations in our dataset. For instance, \n",
    " - **Large Gap Between 75th Percentile and Maximum Value:** The 75th percentile is at 1,015 ms (just over 1 second), while the maximum value is 119,999 ms (about 120 seconds or 2 minutes). This large gap suggests the presence of outliers or very large durations compared to the majority of the data.\n",
    " - **Median Much Lower Than Mean:** The median (50th percentile) value is 136 ms, which is significantly lower than the mean value of 10,310.87 ms. This discrepancy indicates that the data distribution is right-skewed. In other words, while the majority of the bidirectional durations are on the lower end (as evidenced by a relatively low median), there are some durations that are much longer, which increases the average (mean) and indicates the presence of outliers or a long tail to the right of the distribution.\n",
    "\n",
    "These points indicate that the overall interpretation of the distribution being right-skewed with outliers. The data is concentrated on the lower end (short durations), but there are enough high duration values to skew the mean significantly above the median. This pattern is characteristic of a right-skewed distribution.\n",
    "\n",
    "Let's investigate the long durations by assessing the packet inter-arrival time characteristics of the flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5bb357f-8e5f-4444-80b0-aff3753d4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id  packets  duration_ms        label                                                                           splt_piat_ms\n",
      " 66540       10        60184       BENIGN          [0, 61, 0, 0, 62, 1, 0, 59999, 61, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      " 18202       12        60073       BENIGN            [0, 23, 0, 0, 23, 1, 0, 0, 0, 59999, 27, 0, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "483152       12        60236       BENIGN            [0, 78, 0, 0, 78, 1, 0, 0, 0, 59999, 80, 0, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "   668       17        60413       BENIGN             [0, 23, 0, 0, 23, 1, 0, 291, 24, 0, 0, 5, 24, 0, 59999, 23, 0, -1, -1, -1]\n",
      "434935        4        59999       BENIGN       [0, 0, 59999, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "315252       12        60077       BENIGN            [0, 23, 1, 0, 24, 1, 0, 0, 0, 59999, 29, 0, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "441102        4        59999       BENIGN       [0, 0, 59999, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "460485       12        60072       BENIGN            [0, 24, 0, 0, 25, 1, 0, 0, 0, 59997, 25, 0, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "445845       10       119996       BENIGN      [0, 0, 59996, 0, 59541, 46, 1, 0, 412, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "  5806       13        60110       BENIGN             [0, 37, 0, 0, 37, 1, 1, 0, 0, 0, 59995, 38, 1, -1, -1, -1, -1, -1, -1, -1]\n",
      "279394       10        60770       BENIGN      [0, 252, 0, 0, 253, 18, 0, 59991, 256, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      " 83252       13        60102       BENIGN             [0, 32, 1, 0, 32, 1, 0, 0, 0, 0, 59990, 46, 0, -1, -1, -1, -1, -1, -1, -1]\n",
      "390826       10        60063       BENIGN          [0, 24, 0, 0, 24, 1, 0, 59990, 24, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "390827       10        60063       BENIGN          [0, 24, 0, 0, 25, 2, 0, 59988, 24, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "317730       18        60331       BENIGN              [0, 49, 1, 0, 52, 0, 0, 0, 5, 52, 4, 3, 50, 55, 23, 59987, 49, 1, -1, -1]\n",
      "437807       16        60279       BENIGN            [0, 49, 0, 1, 51, 0, 0, 0, 12, 52, 4, 51, 24, 59986, 49, 0, -1, -1, -1, -1]\n",
      " 28867       18        60403       BENIGN             [0, 49, 0, 1, 52, 0, 0, 0, 5, 51, 5, 4, 50, 126, 24, 59986, 50, 0, -1, -1]\n",
      "300613       11        60057       BENIGN           [0, 24, 0, 0, 23, 0, 1, 0, 59986, 23, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "473485       11        60060       BENIGN           [0, 23, 0, 0, 23, 4, 0, 1, 59986, 23, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "300612       11        60057       BENIGN           [0, 24, 0, 0, 22, 1, 1, 0, 59986, 23, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "486376       14        63098       BENIGN           [0, 15, 0, 59985, 15, 0, 3064, 3, 0, 16, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1]\n",
      " 80666       16        60278       BENIGN             [0, 50, 0, 0, 52, 1, 0, 0, 4, 52, 1, 52, 33, 59984, 49, 0, -1, -1, -1, -1]\n",
      "305239       22        60465       BENIGN               [0, 50, 0, 0, 52, 0, 0, 0, 5, 52, 4, 3, 50, 74, 1, 4, 50, 59, 28, 59983]\n",
      "388331       19        61114       BENIGN              [0, 50, 0, 1, 53, 0, 0, 0, 8, 52, 0, 1, 50, 0, 845, 20, 59983, 51, 0, -1]\n",
      " 94160       10        60129       BENIGN         [0, 41, 0, 0, 40, 0, 24, 59983, 41, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "442076       22        61045       BENIGN             [0, 49, 1, 0, 53, 0, 0, 0, 3, 52, 1, 0, 50, 517, 2, 0, 50, 201, 28, 59983]\n",
      "360576       22        60669       BENIGN             [0, 50, 0, 0, 54, 1, 0, 0, 5, 57, 4, 3, 50, 109, 2, 3, 50, 225, 25, 59981]\n",
      "464660       16        60269       BENIGN             [0, 50, 0, 0, 52, 1, 0, 0, 2, 52, 1, 54, 28, 59979, 50, 0, -1, -1, -1, -1]\n",
      "289105       22        60540       BENIGN               [0, 63, 0, 1, 68, 0, 0, 0, 2, 65, 2, 1, 63, 96, 1, 2, 63, 41, 30, 59979]\n",
      "500520       18        60368       BENIGN              [0, 64, 0, 0, 66, 0, 0, 0, 2, 66, 1, 0, 64, 31, 33, 59978, 63, 0, -1, -1]\n",
      "461681       18        60331       BENIGN              [0, 50, 0, 0, 53, 0, 0, 0, 5, 51, 3, 52, 6, 59, 24, 59978, 50, 0, -1, -1]\n",
      "371076       18        60345       BENIGN              [0, 50, 0, 0, 53, 0, 0, 0, 5, 52, 4, 3, 50, 77, 23, 59978, 50, 0, -1, -1]\n",
      "454701       16        60304       BENIGN            [0, 50, 0, 0, 55, 1, 0, 0, 11, 52, 4, 77, 26, 59978, 50, 0, -1, -1, -1, -1]\n",
      " 90431       20        60385       BENIGN               [0, 50, 0, 5, 58, 0, 0, 1, 2, 52, 28, 5, 2, 49, 60, 24, 59977, 71, 0, 1]\n",
      "364211        5       119999 BENIGN - ZPL  [0, 59976, 23, 59977, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "365406        7       119686       BENIGN  [0, 23, 59977, 24, 59584, 51, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "361925        8       119999       BENIGN [0, 23, 59977, 24, 59574, 51, 23, 327, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "328079       16        60269       BENIGN             [0, 50, 0, 0, 52, 0, 0, 0, 2, 54, 2, 51, 31, 59977, 50, 0, -1, -1, -1, -1]\n",
      "307909       22        60511       BENIGN              [0, 50, 0, 0, 53, 0, 0, 0, 2, 51, 2, 1, 51, 139, 3, 1, 49, 58, 24, 59977]\n",
      "361924        7       119653       BENIGN  [0, 23, 59976, 25, 59553, 51, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "import ast  # For converting string representations of lists into actual lists\n",
    "\n",
    "# print(df[df['bidirectional_duration_ms'] == 119999]\n",
    "#       [['src_ip', 'src_port', 'dst_ip', 'dst_port', 'protocol', 'bidirectional_packets',\n",
    "#         'bidirectional_duration_ms', 'label']]\n",
    "#       .head(5).to_string(index=False))\n",
    "\n",
    "# Convert 'splt_piat_ms' from string representations of lists to actual lists\n",
    "df['splt_piat_ms'] = df['splt_piat_ms'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Create a new column for the maximum value in each list\n",
    "df['max_splt_piat_ms'] = df['splt_piat_ms'].apply(max)\n",
    "\n",
    "# Sort the DataFrame based on 'max_splt_piat_ms' in descending order\n",
    "df_sorted = df.sort_values('max_splt_piat_ms', ascending=False)\n",
    "\n",
    "# print(df_sorted['max_splt_piat_ms'].nlargest(8000))\n",
    "\n",
    "# Select the top N rows\n",
    "top_10_rows = df_sorted.head(40)\n",
    "\n",
    "# Create a copy of the DataFrame with renamed columns for display\n",
    "display_df = top_10_rows.rename(columns={\n",
    "    'bidirectional_packets': 'packets',\n",
    "    'bidirectional_duration_ms': 'duration_ms',\n",
    "    # Add other columns to rename here\n",
    "}).copy()\n",
    "\n",
    "# Print the entire row information for these selected rows\n",
    "# print(display_df[['src_ip', 'src_port', 'dst_ip', 'dst_port', 'protocol', 'packets',\n",
    "        # 'duration_ms', 'label', 'splt_piat_ms']].to_string(index=False))\n",
    "\n",
    "print(display_df[['id', 'packets',\n",
    "        'duration_ms', 'label', 'splt_piat_ms']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f31874-fb67-48a0-9c47-ba0e064f448a",
   "metadata": {},
   "source": [
    "From the observed PIAT values, it is evident that there are some notably large PIATs within each of the analyzed flows. These values could suggest that separate flows have been merged into a single flow due to the idle timeout being set to 60 seconds in NFStream.\n",
    "\n",
    "Upon reviewing the default Linux kernel values, a 60-second timeout appears to be a legitimate setting. The variables under [/proc/sys/net/netfilter/nf_conntrack_*](https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt), which relate to the Linux kernel's netfilter connection tracking system—especially when configured with firewall support (iptables/nftables) and connection tracking—support this observation.\n",
    "\n",
    "Moreover, additional default settings in the Linux kernel, such as `tcp_syn_retries`, `tcp_synack_retries`, `tcp_fin_timeout`, and `tcp_keepalive_time` under the [/proc/sys/net/ipv4/*](https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt) variables, corroborate that the configured 60 seconds idle timeout falls within a normal range. These variables, integral to IPv4 networking, enable system administrators to adjust various elements of the IPv4 network stack's behavior, including TCP-specific configurations.\n",
    "\n",
    "Consequently, we classify these flows as valid, where large PIATs signify intervals of inactivity within a TCP connection, aligning with the operational patterns of applications that transmit data in an intermittent manner.\n",
    "\n",
    "As a side note, it is important to recognize that TCP session timeout settings can vary significantly across different firewall vendors. For instance, CheckPoint, a notable firewall vendor, implements [these timeouts](https://support.checkpoint.com/results/sk/sk41248). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dedf1da-04c5-4412-9ffa-373f87732483",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    " - We observed a considerable number of flows with zero packet payload (ZPL), a characteristic not associated with any of the attacks identified in the Wednesday traffic trace.\n",
    " - There is a considerable number of flows that, while being an attack, they also caputre the consequence of a sucessful attack, that is the server starts terminating the connections (via RST/FIN) due to saturation\n",
    " - The Heartbleed attack appears as one prolonged attack that is segmented into multiple flow records by the active timeout setting.\n",
    " - Some flows are repeated with identical 5-tuples across the entire dataset.\n",
    " - There are serveral flows whose PIAT is just a few milliseconds below the idle timeout, which was set to 60 seconds. Upon reviewing the default Linux kernel values, a 60-second timeout appears to be a legitimate setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9dd98-1690-4582-89bb-4a53be2defc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
