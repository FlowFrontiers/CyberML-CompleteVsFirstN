{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56dc5125-aa34-4161-82cd-d16c63750574",
   "metadata": {},
   "source": [
    "## Produce Partial Flows based on Packet Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46830152-0f9c-41f1-afe3-31769b546906",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_DIR = \"datasets\"\n",
    "INPUT_DIR = \"PCAP/deduplicated_reordered\"\n",
    "DAY = \"wednesday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0606ba5c-99ca-4043-89fb-adeb6c0a5fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating flows with NFStream v6.5.4a\n",
      "\n",
      "\n",
      "----- PC=2 -----\n",
      "NFStream generated flows: 6903639\n",
      "Number of flows with packet count = 2: 6370622\n",
      "Flows stored as: wednesday_pc_2.csv\n",
      "\n",
      "\n",
      "----- PC=3 -----\n",
      "NFStream generated flows: 4856398\n",
      "Number of flows with packet count = 3: 3979821\n",
      "Flows stored as: wednesday_pc_3.csv\n",
      "\n",
      "\n",
      "----- PC=4 -----\n",
      "NFStream generated flows: 3887544\n",
      "Number of flows with packet count = 4: 2909344\n",
      "Flows stored as: wednesday_pc_4.csv\n",
      "\n",
      "\n",
      "----- PC=5 -----\n",
      "NFStream generated flows: 3240103\n",
      "Number of flows with packet count = 5: 2245608\n",
      "Flows stored as: wednesday_pc_5.csv\n",
      "\n",
      "\n",
      "----- PC=6 -----\n",
      "NFStream generated flows: 2876898\n",
      "Number of flows with packet count = 6: 1840940\n",
      "Flows stored as: wednesday_pc_6.csv\n",
      "\n",
      "\n",
      "----- PC=7 -----\n",
      "NFStream generated flows: 2622305\n",
      "Number of flows with packet count = 7: 1574465\n",
      "Flows stored as: wednesday_pc_7.csv\n",
      "\n",
      "\n",
      "----- PC=8 -----\n",
      "NFStream generated flows: 2414889\n",
      "Number of flows with packet count = 8: 1379201\n",
      "Flows stored as: wednesday_pc_8.csv\n",
      "\n",
      "\n",
      "----- PC=9 -----\n",
      "NFStream generated flows: 2197225\n",
      "Number of flows with packet count = 9: 1212273\n",
      "Flows stored as: wednesday_pc_9.csv\n",
      "\n",
      "\n",
      "----- PC=10 -----\n",
      "NFStream generated flows: 2048227\n",
      "Number of flows with packet count = 10: 1021347\n",
      "Flows stored as: wednesday_pc_10.csv\n",
      "\n",
      "\n",
      "----- PC=11 -----\n",
      "NFStream generated flows: 1949333\n",
      "Number of flows with packet count = 11: 891292\n",
      "Flows stored as: wednesday_pc_11.csv\n",
      "\n",
      "\n",
      "----- PC=12 -----\n",
      "NFStream generated flows: 1870318\n",
      "Number of flows with packet count = 12: 808396\n",
      "Flows stored as: wednesday_pc_12.csv\n",
      "\n",
      "\n",
      "----- PC=13 -----\n",
      "NFStream generated flows: 1802980\n",
      "Number of flows with packet count = 13: 739874\n",
      "Flows stored as: wednesday_pc_13.csv\n",
      "\n",
      "\n",
      "----- PC=14 -----\n",
      "NFStream generated flows: 1744264\n",
      "Number of flows with packet count = 14: 679955\n",
      "Flows stored as: wednesday_pc_14.csv\n",
      "\n",
      "\n",
      "----- PC=15 -----\n",
      "NFStream generated flows: 1693581\n",
      "Number of flows with packet count = 15: 628248\n",
      "Flows stored as: wednesday_pc_15.csv\n",
      "\n",
      "\n",
      "----- PC=16 -----\n",
      "NFStream generated flows: 1651190\n",
      "Number of flows with packet count = 16: 583016\n",
      "Flows stored as: wednesday_pc_16.csv\n",
      "\n",
      "\n",
      "----- PC=17 -----\n",
      "NFStream generated flows: 1613800\n",
      "Number of flows with packet count = 17: 545314\n",
      "Flows stored as: wednesday_pc_17.csv\n",
      "\n",
      "\n",
      "----- PC=18 -----\n",
      "NFStream generated flows: 1581078\n",
      "Number of flows with packet count = 18: 511933\n",
      "Flows stored as: wednesday_pc_18.csv\n",
      "\n",
      "\n",
      "----- PC=19 -----\n",
      "NFStream generated flows: 1551643\n",
      "Number of flows with packet count = 19: 482081\n",
      "Flows stored as: wednesday_pc_19.csv\n",
      "\n",
      "\n",
      "----- PC=20 -----\n",
      "NFStream generated flows: 1524091\n",
      "Number of flows with packet count = 20: 454492\n",
      "Flows stored as: wednesday_pc_20.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import logging\n",
    "from nfstream import NFPlugin, NFStreamer\n",
    "import nfstream\n",
    "# from labeller import cicids2017\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# set up logging\n",
    "def setup_logging(log_filename=\"generate-n-pc-flows.log\"):\n",
    "    with open(log_filename, \"w\"):  # Use 'w' to clear the existing log file, if it exists\n",
    "        pass  # Do nothing, just open and close to clear the file\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(message)s\",\n",
    "        handlers=[logging.FileHandler(log_filename), logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "def consistent_hash(value):\n",
    "    # This function converts a value into a consistent hash.\n",
    "    return hashlib.sha256(value.encode()).hexdigest()\n",
    "    \n",
    "\n",
    "class PayloadManager(NFPlugin):\n",
    "    \"\"\"Manages the payload data for network flows.\"\"\"\n",
    "\n",
    "    def on_init(self, packet, flow):\n",
    "        # Initialize payload sizes based on the packet direction.\n",
    "        flow.udps.src2dst_payload = packet.payload_size if packet.direction == 0 else 0\n",
    "        flow.udps.dst2src_payload = packet.payload_size if packet.direction == 1 else 0\n",
    "\n",
    "    def on_update(self, packet, flow):\n",
    "        # Update payload sizes based on the packet direction.\n",
    "        flow.udps.src2dst_payload += packet.payload_size if packet.direction == 0 else 0\n",
    "        flow.udps.dst2src_payload += packet.payload_size if packet.direction == 1 else 0\n",
    "\n",
    "\n",
    "class FlowExpirationManager(NFPlugin):\n",
    "    \"\"\"Manages the expiration policy for TCP flows.\"\"\"\n",
    "\n",
    "    def on_init(self, packet, flow):\n",
    "        # Set the expiration ID based on TCP rst or fin flags.\n",
    "        if packet.rst or packet.fin:\n",
    "            flow.expiration_id = -1\n",
    "\n",
    "    def on_update(self, packet, flow):\n",
    "        # Update expiration policy based on TCP rst or fin flags.\n",
    "        if packet.rst or packet.fin:\n",
    "            flow.expiration_id = -1\n",
    "\n",
    "\n",
    "class FlowLabelManager(NFPlugin):\n",
    "    \"\"\"Labels flows upon expiration.\"\"\"\n",
    "\n",
    "    def __init__(self, day):\n",
    "        self.day = day\n",
    "\n",
    "    def on_expire(self, flow):\n",
    "        # Assign a label to the flow and clean up payloads.\n",
    "        flow.udps.label = cicids2017(\n",
    "            self.day, flow, label_reverse=True, signal_reverse=False\n",
    "        )\n",
    "        self.cleanup_payload(flow)\n",
    "\n",
    "    @staticmethod\n",
    "    def cleanup_payload(flow):\n",
    "        # Clean up payload data from the flow.\n",
    "        if hasattr(flow.udps, \"src2dst_payload\"):\n",
    "            del flow.udps.src2dst_payload\n",
    "        if hasattr(flow.udps, \"dst2src_payload\"):\n",
    "            del flow.udps.dst2src_payload\n",
    "\n",
    "\n",
    "class PacketCountManager(NFPlugin):\n",
    "    \"\"\"Expire flows on specific packet count.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_packets):\n",
    "        self.max_packets = max_packets\n",
    "\n",
    "    def on_update(self, packet, flow):\n",
    "        # Check for expiration\n",
    "        if flow.bidirectional_packets == self.max_packets:\n",
    "            flow.expiration_id = -1  # Mark for expiration in NFStream\n",
    "\n",
    "class HashManager(NFPlugin):\n",
    "    \"\"\"Calculate forward and backward hashes.\"\"\"\n",
    "    \n",
    "    def on_init(self, packet, flow):\n",
    "        # Initialize packet count and compute initial hashes\n",
    "        flow.udps.flow_key_hash = consistent_hash(f\"{packet.src_ip}-{packet.src_port}-{packet.dst_ip}-{packet.dst_port}-{packet.protocol}-{flow.bidirectional_first_seen_ms}\")\n",
    "\n",
    "\n",
    "def process_files_in_directory(input_dir: str, day: str, output_dir: str, Ns: list):\n",
    "    \"\"\"Process all PCAP files in a directory and output to another directory.\"\"\"\n",
    "\n",
    "    BPF = \"ip and (ip proto \\\\tcp or \\\\udp)\"  # only ipv4 tcp and udp traffic to capture\n",
    "\n",
    "    for n in Ns:\n",
    "        input_file = os.path.join(input_dir, f\"rd{day.capitalize()}.pcap\")\n",
    "        if os.path.isfile(input_file):\n",
    "            output_file = os.path.join(output_dir, f\"{day}_pc_{n}.csv\")\n",
    "\n",
    "            logging.info(f\"----- PC={n} -----\")\n",
    "            # logging.info(f\"Processing {input_file} into {output_file}\")\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            streamer = NFStreamer(\n",
    "                  source=input_file\n",
    "                , decode_tunnels=False                                # Default: True\n",
    "                , bpf_filter=BPF                                      # Default: None\n",
    "                , promiscuous_mode=True                               # Default: True\n",
    "                , snapshot_length=1536                                # Default: 1536\n",
    "                , idle_timeout=60                                     # Default: 120\n",
    "                , active_timeout=18000                                # Default: 1800\n",
    "                , accounting_mode=1                                   # Default: 0\n",
    "                , udps=[                                              # Default: None\n",
    "                    FlowExpirationManager(),\n",
    "                    # PayloadManager(),\n",
    "                    # FlowLabelManager(day.capitalize()),\n",
    "                    HashManager(),\n",
    "                    PacketCountManager(n)\n",
    "                ]      \n",
    "                , n_dissections=0                                     # Default: 20\n",
    "                , statistical_analysis=True                           # Default: False\n",
    "                , splt_analysis=20                                     # Default: 0\n",
    "                , n_meters=1                                          # Default: 0\n",
    "                , performance_report=0                                # Default: 0\n",
    "            )\n",
    "\n",
    "            # Convert the stream to a DataFrame\n",
    "            df = streamer.to_pandas(columns_to_anonymize=[])\n",
    "            logging.info(f\"NFStream generated flows: {len(df)}\")\n",
    "\n",
    "            end = time.time()\n",
    "            processing_time = end - start\n",
    "            delta = timedelta(seconds=processing_time)\n",
    "            # logging.info(f\"Time required to generate flows: {str(delta)}\")\n",
    "\n",
    "            # Remove rows where 'bidirectional_packets' does not equal n packets\n",
    "            df_filtered = df[df['bidirectional_packets'] == n]\n",
    "            logging.info(f\"Number of flows with packet count = {n}: {len(df_filtered)}\")\n",
    "            df = df_filtered\n",
    "            \n",
    "            # Save the filtered DataFrame to a CSV file\n",
    "            df.rename(columns={\n",
    "                                # \"udps.label\": \"label\",\n",
    "                                \"udps.flow_key_hash\": \"flow_key_hash\"\n",
    "                               }, inplace=True)\n",
    "            df.to_csv(output_file, index=False)\n",
    "\n",
    "            logging.info(f\"Flows stored as: {day}_pc_{n}.csv\")\n",
    "            logging.info(f\"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = INPUT_DIR\n",
    "    output_dir = CSV_DIR\n",
    "    day = DAY\n",
    "    Ns = range(2,20+1)\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    setup_logging()\n",
    "\n",
    "    logging.info(f\"Generating flows with NFStream v{nfstream.__version__}\")\n",
    "    logging.info(f\"\\n\")\n",
    "\n",
    "    process_files_in_directory(input_dir, day, output_dir, Ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55081040-e404-4431-aa58-3885a27cd01a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
