{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5ee513-10e9-4fe1-aac0-f58c48b582b7",
   "metadata": {},
   "source": [
    "## Producing Complete Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212796df-b8eb-49a5-a51b-df08df39f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"results\"\n",
    "CSV_DIR = \"datasets\"\n",
    "INPUT_DIR = \"PCAP/deduplicated_reordered\"\n",
    "DAY = \"wednesday\"\n",
    "\n",
    "SCENARIO = \"cf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ddc14-a466-4ed6-9ea4-0728ead7c47b",
   "metadata": {},
   "source": [
    "## For this flow measurement \n",
    " - we set `idle timeout` to `60 seconds`\n",
    " - we set `actie timeout` to `18000 seconds (5 hours)` to prevent long flows being separated into multiple entries\n",
    " - we expire flows on the first RST/FIN packet. With this, we want to avoid observing the consequence of the attacks, which manifests in the connections being terminated. Otherwise, we could not focus on solely on the attack characteristics themselves.\n",
    " - we drop all the flows which are being 'cut' from the active flows by expiring them on the first RST/FIN flag.\n",
    " - we make flow start part of the flow ID calculated as a hash to better identify flows later with their subflow counterparts.\n",
    " - we drop all flows associated with Heartbleed attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c76fcc-cd5c-4edd-bbe8-5400392dd562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating flows with NFStream v6.5.4a\n",
      "\n",
      "\n",
      "NFStream generated flows: 1074383\n",
      "Flows filtered based on RST/FIN: 219536\n",
      "Number of  complete flows: 854847\n",
      "Flows stored as: wednesday_cf.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import logging\n",
    "from nfstream import NFPlugin, NFStreamer\n",
    "import nfstream\n",
    "from labeller import cicids2017\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# set up logging\n",
    "def setup_logging(log_filename=\"generate-complete-flows.log\"):\n",
    "    with open(log_filename, \"w\"):  # Use 'w' to clear the existing log file, if it exists\n",
    "        pass  # Do nothing, just open and close to clear the file\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(message)s\",\n",
    "        handlers=[logging.FileHandler(log_filename), logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "\n",
    "def consistent_hash(value):\n",
    "    # This function converts a value into a consistent hash.\n",
    "    return hashlib.sha256(value.encode()).hexdigest()\n",
    "    \n",
    "\n",
    "class PayloadManager(NFPlugin):\n",
    "    \"\"\"Manages the payload data for network flows.\"\"\"\n",
    "\n",
    "    def on_init(self, packet, flow):\n",
    "        # Initialize payload sizes based on the packet direction.\n",
    "        flow.udps.src2dst_payload = packet.payload_size if packet.direction == 0 else 0\n",
    "        flow.udps.dst2src_payload = packet.payload_size if packet.direction == 1 else 0\n",
    "\n",
    "    def on_update(self, packet, flow):\n",
    "        # Update payload sizes based on the packet direction.\n",
    "        flow.udps.src2dst_payload += packet.payload_size if packet.direction == 0 else 0\n",
    "        flow.udps.dst2src_payload += packet.payload_size if packet.direction == 1 else 0\n",
    "\n",
    "\n",
    "class FlowExpirationManager(NFPlugin):\n",
    "    \"\"\"Manages the expiration policy for TCP flows.\"\"\"\n",
    "\n",
    "    def on_init(self, packet, flow):\n",
    "        # Set the expiration ID based on TCP rst or fin flags.\n",
    "        if packet.rst or packet.fin:\n",
    "            flow.expiration_id = -1\n",
    "\n",
    "    def on_update(self, packet, flow):\n",
    "        # Update expiration policy based on TCP rst or fin flags.\n",
    "        if packet.rst or packet.fin:\n",
    "            flow.expiration_id = -1\n",
    "\n",
    "\n",
    "class FlowLabelManager(NFPlugin):\n",
    "    \"\"\"Labels flows upon expiration.\"\"\"\n",
    "\n",
    "    def __init__(self, day):\n",
    "        self.day = day\n",
    "\n",
    "    def on_expire(self, flow):\n",
    "        # Assign a label to the flow and clean up payloads.\n",
    "        flow.udps.label = cicids2017(\n",
    "            self.day, flow, label_reverse=True, signal_reverse=False\n",
    "        )\n",
    "        self.cleanup_payload(flow)\n",
    "\n",
    "    @staticmethod\n",
    "    def cleanup_payload(flow):\n",
    "        # Clean up payload data from the flow.\n",
    "        if hasattr(flow.udps, \"src2dst_payload\"):\n",
    "            del flow.udps.src2dst_payload\n",
    "        if hasattr(flow.udps, \"dst2src_payload\"):\n",
    "            del flow.udps.dst2src_payload\n",
    "\n",
    "\n",
    "class HashManager(NFPlugin):\n",
    "    \"\"\"Calculate forward and backward hashes.\"\"\"\n",
    "    def on_init(self, packet, flow):\n",
    "        flow.udps.flow_key_hash = consistent_hash(f\"{packet.src_ip}-{packet.src_port}-{packet.dst_ip}-{packet.dst_port}-{packet.protocol}-{flow.bidirectional_first_seen_ms}\")\n",
    "\n",
    "\n",
    "def process_files_in_directory(input_dir: str, day: str, output_dir: str):\n",
    "    \"\"\"Process all PCAP files in a directory and output to another directory.\"\"\"\n",
    "\n",
    "    BPF = \"ip and (ip proto \\\\tcp or \\\\udp)\"  # only ipv4 tcp and udp traffic to capture\n",
    "\n",
    "    input_file = os.path.join(input_dir, f\"rd{day.capitalize()}.pcap\")\n",
    "    if os.path.isfile(input_file):\n",
    "        output_file = os.path.join(output_dir, f\"{day}_{SCENARIO}_wzpl.csv\")\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        streamer = NFStreamer(\n",
    "              source=input_file\n",
    "            , decode_tunnels=False                                # Default: True\n",
    "            , bpf_filter=BPF                                      # Default: None\n",
    "            , promiscuous_mode=True                               # Default: True\n",
    "            , snapshot_length=1536                                # Default: 1536\n",
    "            , idle_timeout=60                                     # Default: 120\n",
    "            , active_timeout=18000                                # Default: 1800\n",
    "            , accounting_mode=1                                   # Default: 0\n",
    "            , udps=[                                              # Default: None\n",
    "                FlowExpirationManager(),\n",
    "                PayloadManager(),\n",
    "                HashManager(),\n",
    "                FlowLabelManager(day.capitalize())\n",
    "            ]      \n",
    "            , n_dissections=0                                     # Default: 20\n",
    "            , statistical_analysis=True                           # Default: False\n",
    "            , splt_analysis=20                                     # Default: 0\n",
    "            , n_meters=1                                          # Default: 0\n",
    "            , performance_report=0                                # Default: 0\n",
    "        )\n",
    "\n",
    "        # Convert the stream to a DataFrame\n",
    "        df = streamer.to_pandas(columns_to_anonymize=[])\n",
    "        logging.info(f\"NFStream generated flows: {len(df)}\")\n",
    "\n",
    "        end = time.time()\n",
    "        processing_time = end - start\n",
    "        delta = timedelta(seconds=processing_time)\n",
    "        # logging.info(f\"Time required to generate flows: {str(delta)}\")\n",
    "\n",
    "        # Filter the DataFrame\n",
    "        df_filtered = df[\n",
    "            ~(\n",
    "                (df[\"bidirectional_packets\"] == 1)\n",
    "                & (\n",
    "                    (df[\"bidirectional_rst_packets\"] == 1)\n",
    "                    | (df[\"bidirectional_fin_packets\"] == 1)\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "        logging.info(f\"Flows filtered based on RST/FIN: {len(df)-len(df_filtered)}\")\n",
    "        df = df_filtered\n",
    "        logging.info(f\"Number of complete flows: {len(df)}\")\n",
    "\n",
    "        # Save the filtered DataFrame to a CSV file\n",
    "        df.rename(columns={\n",
    "                           \"udps.label\": \"label\",\n",
    "                           \"udps.flow_key_hash\": \"flow_key_hash\"\n",
    "                          }, inplace=True)\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "        logging.info(f\"Flows stored as: {day}_{SCENARIO}.csv\")\n",
    "        logging.info(f\"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = INPUT_DIR\n",
    "    output_dir = CSV_DIR\n",
    "    day = DAY\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    setup_logging()\n",
    "\n",
    "    logging.info(f\"Generating flows with NFStream v{nfstream.__version__}\")\n",
    "    logging.info(f\"\\n\")\n",
    "    \n",
    "    process_files_in_directory(input_dir, day, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3991051-44ce-439e-9e1e-dc2efbd671d2",
   "metadata": {},
   "source": [
    "## Show dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34071ce5-af48-48a5-9288-0135b2559e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        DS                TOTAL               BENIGN             ANOMALY                 Anomaly breakdown          \n",
      "------------------  ------------------  ------------------  ------------------  ------------------------------------\n",
      "    wednesday             854847              327470              527377                                        \n",
      "                                                                                BENIGN - ZPL              122828\n",
      "                                                                                DoS GoldenEye               7917\n",
      "                                                                                DoS GoldenEye - ZPL         7483\n",
      "                                                                                DoS Hulk                  158680\n",
      "                                                                                DoS Hulk - ZPL            216413\n",
      "                                                                                DoS Slowhttptest            3707\n",
      "                                                                                DoS Slowhttptest - ZPL      2988\n",
      "                                                                                DoS Slowloris               5683\n",
      "                                                                                DoS Slowloris - ZPL         1676\n",
      "                                                                                Heartbleed                     2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "day = DAY\n",
    "\n",
    "# Initializing the table.\n",
    "header = [\"DS\", \"TOTAL\", \"BENIGN\", \"ANOMALY\", \"Anomaly breakdown\"]\n",
    "rowh = \"{:^18}  \"*(len(header)-1) + \"{:^36}\"\n",
    "row  = \"{:^18}  \"*(len(header)-1) + \"{:<26}\" + \"{:>6}\"\n",
    "sep  = [\"-\"*18]*(len(header)-1) + [\"-\"*36]\n",
    "print(rowh.format(*header))\n",
    "print(rowh.format(*sep))\n",
    "\n",
    "csv = pd.read_csv(os.path.join(CSV_DIR,f\"{day}_{SCENARIO}_wzpl.csv\"))\n",
    "\n",
    "TOTAL = len(csv)\n",
    "BENIGN = len(csv[csv[\"label\"] == \"BENIGN\"])\n",
    "ANOMALY = len(csv[(csv[\"label\"] != \"BENIGN\") & (csv[\"label\"] != \"NaN\")])\n",
    "\n",
    "print(row.format(DAY, TOTAL, BENIGN, ANOMALY, \"\", \"\"))\n",
    "for label in sorted(csv[\"label\"].unique().tolist()):\n",
    "    if label == \"BENIGN\":\n",
    "        continue\n",
    "    print(row.format(\"\",\"\",\"\",\"\", label, len(csv[csv[\"label\"] == label])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c4d0f-59b6-4fcb-866d-55c78866ca85",
   "metadata": {},
   "source": [
    "## Filter out all flows with zero payload and drop Heartbleed flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b5d757-6ac3-498d-81f6-e2de35582654",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = csv[~csv['label'].str.contains(\"ZPL\")]\n",
    "filtered_df = filtered_df[filtered_df['label'] != 'Heartbleed'].copy()\n",
    "filtered_df.to_csv(os.path.join(CSV_DIR, f\"{day}_{SCENARIO}_wozpl.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd2f54-1168-4495-8f12-3e321378a582",
   "metadata": {},
   "source": [
    "## Show dataset distribution after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf18da4-5141-4744-a945-ea38e09454f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        DS                TOTAL               BENIGN             ANOMALY                 Anomaly breakdown          \n",
      "------------------  ------------------  ------------------  ------------------  ------------------------------------\n",
      "    wednesday             503457              327470              175987                                        \n",
      "                                                                                DoS GoldenEye               7917\n",
      "                                                                                DoS Hulk                  158680\n",
      "                                                                                DoS Slowhttptest            3707\n",
      "                                                                                DoS Slowloris               5683\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "day = DAY\n",
    "\n",
    "# Initializing the table.\n",
    "header = [\"DS\", \"TOTAL\", \"BENIGN\", \"ANOMALY\", \"Anomaly breakdown\"]\n",
    "rowh = \"{:^18}  \"*(len(header)-1) + \"{:^36}\"\n",
    "row  = \"{:^18}  \"*(len(header)-1) + \"{:<26}\" + \"{:>6}\"\n",
    "sep  = [\"-\"*18]*(len(header)-1) + [\"-\"*36]\n",
    "print(rowh.format(*header))\n",
    "print(rowh.format(*sep))\n",
    "\n",
    "csv = pd.read_csv(os.path.join(CSV_DIR,f\"{day}_{SCENARIO}_wozpl.csv\"))\n",
    "\n",
    "TOTAL = len(csv)\n",
    "BENIGN = len(csv[csv[\"label\"] == \"BENIGN\"])\n",
    "ANOMALY = len(csv[(csv[\"label\"] != \"BENIGN\") & (csv[\"label\"] != \"NaN\")])\n",
    "\n",
    "print(row.format(DAY, TOTAL, BENIGN, ANOMALY, \"\", \"\"))\n",
    "for label in sorted(csv[\"label\"].unique().tolist()):\n",
    "    if label == \"BENIGN\":\n",
    "        continue\n",
    "    print(row.format(\"\",\"\",\"\",\"\", label, len(csv[csv[\"label\"] == label])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0a8a8-3730-42e0-8048-0610784792cf",
   "metadata": {},
   "source": [
    "## Show FIN and RST statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbd29a-4c2a-48f5-8d96-e387703f2d4f",
   "metadata": {},
   "source": [
    "### Count of flows whose FIN or RST count is higher than 1, 2, and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d85877c-9a7c-4435-b9a5-0afe6aa3063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "   DS        TOTAL     BENIGN     ANOMALY    T_FIN>1    B_FIN>1    A_FIN>1    T_RST>1    B_RST>1    A_RST>1   \n",
      "---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  \n",
      "wednesday   503457     327470     175987        0          0          0          0          0          0      \n",
      "\n",
      "\n",
      "   DS        TOTAL     BENIGN     ANOMALY    T_FIN>2    B_FIN>2    A_FIN>2    T_RST>2    B_RST>2    A_RST>2   \n",
      "---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  \n",
      "wednesday   503457     327470     175987        0          0          0          0          0          0      \n",
      "\n",
      "\n",
      "   DS        TOTAL     BENIGN     ANOMALY    T_FIN>3    B_FIN>3    A_FIN>3    T_RST>3    B_RST>3    A_RST>3   \n",
      "---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  \n",
      "wednesday   503457     327470     175987        0          0          0          0          0          0      \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for N in range(1,1+3):\n",
    "    # Initializing the table.\n",
    "    header = [\"DS\", \"TOTAL\", \"BENIGN\", \"ANOMALY\",\n",
    "              f\"T_FIN>{N}\", f\"B_FIN>{N}\", f\"A_FIN>{N}\",\n",
    "              f\"T_RST>{N}\", f\"B_RST>{N}\", f\"A_RST>{N}\"]\n",
    "    rowh = \"{:^9}  \"*(len(header))\n",
    "    row = \"{:^9}  \"*(len(header))\n",
    "    sep = [\"-\"*9]*(len(header)) + [\"-\"*39]\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(rowh.format(*header)) \n",
    "    print(rowh.format(*sep))\n",
    "    \n",
    "    TOTAL = len(csv)\n",
    "    FIN_GT = len(csv[csv[\"bidirectional_fin_packets\"] > N])\n",
    "    RST_GT = len(csv[csv[\"bidirectional_rst_packets\"] > N])\n",
    "    BENIGN = len(csv[csv[\"label\"] == \"BENIGN\"])\n",
    "    BENIGN_FIN_GT = len(csv[(csv[\"label\"] == \"BENIGN\") & (csv[\"bidirectional_fin_packets\"] > N)])\n",
    "    BENIGN_RST_GT = len(csv[(csv[\"label\"] == \"BENIGN\") & (csv[\"bidirectional_rst_packets\"] > N)])\n",
    "    ANOMALY = len(csv[(csv[\"label\"] != \"BENIGN\")])\n",
    "    ANOMALY_FIN_GT = len(csv[(csv[\"label\"] != \"BENIGN\") & (csv[\"bidirectional_fin_packets\"] > N)])\n",
    "    ANOMALY_RST_GT = len(csv[(csv[\"label\"] != \"BENIGN\") & (csv[\"bidirectional_rst_packets\"] > N)])\n",
    "    \n",
    "    print(row.format(day, TOTAL, BENIGN, ANOMALY,\n",
    "                     FIN_GT, BENIGN_FIN_GT, ANOMALY_FIN_GT,\n",
    "                     RST_GT, BENIGN_RST_GT, ANOMALY_RST_GT))\n",
    "    \n",
    "    # print(\"-\"*108)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29075605-0230-4cc7-b0a6-fea6dd896095",
   "metadata": {},
   "source": [
    "## Show statistics for the FIN and RST counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad8c1a9-e8d2-47e5-a4f4-720a3d814100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       bidirectional_fin_packets  bidirectional_rst_packets\n",
      "count              503457.000000              503457.000000\n",
      "mean                    0.539005                   0.036164\n",
      "std                     0.498477                   0.186698\n",
      "min                     0.000000                   0.000000\n",
      "25%                     0.000000                   0.000000\n",
      "50%                     1.000000                   0.000000\n",
      "75%                     1.000000                   0.000000\n",
      "max                     1.000000                   1.000000\n"
     ]
    }
   ],
   "source": [
    "print(csv[['bidirectional_fin_packets', 'bidirectional_rst_packets']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2abcf34-d014-464b-b591-7238a63776a6",
   "metadata": {},
   "source": [
    "## Show top 5 FIN and RST counts with the higher occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb8d488-86d9-4c03-ba95-38443f6e2e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for wednesday:\n",
      "\n",
      "Top 5 Value Counts for FIN Packets:\n",
      "1    271366\n",
      "0    232091\n",
      "\n",
      "Top 5 Value Counts for RST Packets:\n",
      "0    485250\n",
      "1     18207\n"
     ]
    }
   ],
   "source": [
    "N=5\n",
    "\n",
    "# # Get descriptive statistics\n",
    "print(f\"Statistics for {day}:\")\n",
    "# stats = csv[['bidirectional_fin_packets', 'bidirectional_rst_packets']].describe(percentiles=[0.5, 0.75, 0.9, 0.95, 1.0])\n",
    "# print(stats)\n",
    "\n",
    "# Print the top N value counts for each column without the index name and series description\n",
    "fin_counts = csv['bidirectional_fin_packets'].value_counts().head(N).to_string(header=False, index=True)\n",
    "rst_counts = csv['bidirectional_rst_packets'].value_counts().head(N).to_string(header=False, index=True)\n",
    "\n",
    "print(f\"\\nTop {N} Value Counts for FIN Packets:\")\n",
    "print(fin_counts)\n",
    "\n",
    "print(f\"\\nTop {N} Value Counts for RST Packets:\")\n",
    "print(rst_counts)\n",
    "\n",
    "# print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c315144-0dfd-4550-9e79-1f888501f8fe",
   "metadata": {},
   "source": [
    "## Show flow expiration statistics\n",
    "\n",
    "expiration_id =\n",
    " - 0 for idle timeout,\n",
    " - 1 for active timeout, or\n",
    " - -1 for custom expiration.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ea7593f-8fd3-4583-93a7-b36997fb1fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expiration_id\n",
      "-1    289573\n",
      " 0    213884\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = csv\n",
    "print(df['expiration_id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fafadc-5a58-4c76-aa76-aaed0c409bc7",
   "metadata": {},
   "source": [
    "## Show time related statistics for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29f33dc6-d927-4165-a94a-9ae58c5cb859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats about bidirectional_duration_ms:\n",
      "count    5.034570e+05\n",
      "mean     1.041577e+04\n",
      "std      3.887668e+04\n",
      "min      0.000000e+00\n",
      "25%      6.000000e+00\n",
      "50%      7.500000e+01\n",
      "75%      3.550000e+02\n",
      "max      1.652774e+06\n",
      "\n",
      "Show top N longest flow values and their occurence:\n",
      "1652774    1\n",
      "1551220    1\n",
      "1533020    1\n",
      "1526352    1\n",
      "1517830    1\n",
      "1517829    5\n",
      "1516166    1\n",
      "1464626    1\n",
      "1453045    1\n",
      "1438665    1\n",
      "\n",
      "Show limited info of flow with the longest duration:\n",
      "       src_ip  src_port       dst_ip  dst_port  protocol  bidirectional_duration_ms  label\n",
      "192.168.10.17     60326 52.79.87.176       443         6                    1652774 BENIGN\n"
     ]
    }
   ],
   "source": [
    "print(\"Stats about bidirectional_duration_ms:\")\n",
    "print(df['bidirectional_duration_ms'].describe().to_string())\n",
    "\n",
    "print(\"\\nShow top N longest flow values and their occurence:\")\n",
    "print(df['bidirectional_duration_ms'].value_counts().sort_index(ascending=False).head(10).to_string(header=False))\n",
    "\n",
    "print(\"\\nShow limited info of flow with the longest duration:\")\n",
    "print(df[df['bidirectional_duration_ms'] == df['bidirectional_duration_ms'].max()]\n",
    "      [['src_ip', 'src_port', 'dst_ip', 'dst_port', 'protocol', 'bidirectional_duration_ms', 'label']]\n",
    "      .head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b981f3-006a-4869-a089-e285b970d304",
   "metadata": {},
   "source": [
    "## Show the number of flows whose unique ID appears more than once in the DS\n",
    "\n",
    "Such flows are esentially repeated across the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b7aad5-747a-4025-a32a-8b6278e6dd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Label  Hash_More_Than_One\n",
      "0            BENIGN                 843\n",
      "1     DoS Slowloris                   0\n",
      "2  DoS Slowhttptest                   0\n",
      "3          DoS Hulk                   0\n",
      "4     DoS GoldenEye                   0\n"
     ]
    }
   ],
   "source": [
    "# Prepare an empty list to store results\n",
    "results_list = []\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = df['label'].unique()\n",
    "\n",
    "# Iterate through each unique label\n",
    "for label in unique_labels:\n",
    "    # Filter the dataset for the current label\n",
    "    df_label = df[df['label'] == label]\n",
    "    \n",
    "    # Count the occurrences of each unique value in the 'forward_hash' and 'udps.backward_hash' columns\n",
    "    value_counts = df_label['flow_key_hash'].value_counts()\n",
    "\n",
    "    # print(value_counts_f.head(10))\n",
    "    \n",
    "    # Count how many unique values have more than one occurrence\n",
    "    more_than_one_unique = sum(value_counts > 1)\n",
    "    \n",
    "    # Append the results for the current label to the results list\n",
    "    results_list.append({'Label': label, \n",
    "                         'Hash_More_Than_One': more_than_one_unique\n",
    "                        })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Print the result\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f672f583-5b47-4825-a624-44f8e04c68c4",
   "metadata": {},
   "source": [
    "## Let's drop the duplicate entries for the BENIGN flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa37a9d5-c2ab-4fb0-a84b-7ada21059914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Label  Hash_More_Than_One\n",
      "0            BENIGN                   0\n",
      "1     DoS Slowloris                   0\n",
      "2  DoS Slowhttptest                   0\n",
      "3          DoS Hulk                   0\n",
      "4     DoS GoldenEye                   0\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame by 'bidirectional_first_seen_ms' in ascending order\n",
    "df_sorted = df.sort_values(by='id')\n",
    "\n",
    "# Now drop duplicates, keeping the first occurrence (which is now the earliest)\n",
    "df_unique = df_sorted.drop_duplicates(subset=['flow_key_hash'], keep='first')\n",
    "\n",
    "# Prepare an empty list to store results\n",
    "results_list = []\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = df_unique['label'].unique()\n",
    "\n",
    "# Iterate through each unique label\n",
    "for label in unique_labels:\n",
    "    # Filter the dataset for the current label\n",
    "    df_label = df_unique[df_unique['label'] == label]\n",
    "    \n",
    "    # Count the occurrences of each unique value in the 'forward_hash' and 'udps.backward_hash' columns\n",
    "    value_counts = df_label['flow_key_hash'].value_counts()\n",
    "\n",
    "    # print(value_counts_f.head(10))\n",
    "    \n",
    "    # Count how many unique values have more than one occurrence\n",
    "    more_than_one_unique = sum(value_counts > 1)\n",
    "    \n",
    "    # Append the results for the current label to the results list\n",
    "    results_list.append({'Label': label, \n",
    "                         'Hash_More_Than_One': more_than_one_unique\n",
    "                        })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Print the result\n",
    "print(results_df)\n",
    "\n",
    "df = df_unique\n",
    "df.to_csv(os.path.join(CSV_DIR, f\"{DAY}_{SCENARIO}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5c59c-6295-4e77-9d97-71c94d8c5f5c",
   "metadata": {},
   "source": [
    "## Insights into the temporal distribution of flows for each label based on the `bidirectional_first_seen_ms` timestamp\n",
    "\n",
    "The output is a new DataFrame (`stats`) where each row corresponds to a unique label from our data, and the columns include:\n",
    "\n",
    "- `label`: The unique identifier for each group of flows.\n",
    "- `min`: The minimum time difference between consecutive flows within the same label group.\n",
    "- `max`: The maximum time difference between consecutive flows within the same label group.\n",
    "- `mean`: The average time difference between consecutive flows within the same label group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c179f77-bd64-4a0a-afd8-ec2c2096a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              label  min       max        mean\n",
      "0            BENIGN  0.0   93789.0   93.308881\n",
      "1     DoS GoldenEye  0.0  158048.0   76.513896\n",
      "2          DoS Hulk  0.0   52243.0    6.877520\n",
      "3  DoS Slowhttptest  0.0  174425.0  359.253643\n",
      "4     DoS Slowloris  0.0    9120.0  230.603133\n",
      "\n",
      "The same in a human readable form:\n",
      "\n",
      "              label      min            max       mean\n",
      "0            BENIGN  0.00 ms   1.56 minutes   93.31 ms\n",
      "1     DoS GoldenEye  0.00 ms   2.63 minutes   76.51 ms\n",
      "2          DoS Hulk  0.00 ms  52.24 seconds    6.88 ms\n",
      "3  DoS Slowhttptest  0.00 ms   2.91 minutes  359.25 ms\n",
      "4     DoS Slowloris  0.00 ms   9.12 seconds  230.60 ms\n"
     ]
    }
   ],
   "source": [
    "# Sort the dataframe by label and then by first_seen_ms to ensure the order\n",
    "df_sorted = df.sort_values(by=['label', 'bidirectional_first_seen_ms'])\n",
    "\n",
    "# Calculate the difference in 'bidirectional_first_seen_ms' between subsequent rows within each 'label'\n",
    "df_sorted['time_diff'] = df_sorted.groupby('label')['bidirectional_first_seen_ms'].diff()\n",
    "\n",
    "# Now, group by 'label' and calculate min, max, and mean differences\n",
    "stats = df_sorted.groupby('label')['time_diff'].agg(['min', 'max', 'mean']).reset_index()\n",
    "\n",
    "# Print the resulting statistics for each label\n",
    "print(stats)\n",
    "\n",
    "print(\"\\nThe same in a human readable form:\\n\")\n",
    "\n",
    "# Convert milliseconds to more readable units\n",
    "def convert_to_readable_time(ms):\n",
    "    if pd.isna(ms):  # Check for NaN values\n",
    "        return 'N/A'  # Return 'N/A' for NaN values\n",
    "    if ms < 1000:\n",
    "        return f\"{ms:.2f} ms\"  # Keep milliseconds if less than one second\n",
    "    elif ms < 60000:\n",
    "        return f\"{ms / 1000:.2f} seconds\"  # Convert to seconds if less than one minute\n",
    "    elif ms < 3600000:\n",
    "        return f\"{ms / 60000:.2f} minutes\"  # Convert to minutes if less than one hour\n",
    "    else:\n",
    "        return f\"{ms / 3600000:.2f} hours\"  # Convert to hours otherwise\n",
    "\n",
    "# Apply the conversion to each time column\n",
    "stats['min'] = stats['min'].apply(convert_to_readable_time)\n",
    "stats['max'] = stats['max'].apply(convert_to_readable_time)\n",
    "stats['mean'] = stats['mean'].apply(convert_to_readable_time)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16915eee-d8f5-4c08-bb52-fab74b104ec8",
   "metadata": {},
   "source": [
    "## Show final dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7c33301-e4ca-403c-9ae3-c681df8ce015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        DS                TOTAL               BENIGN             ANOMALY                 Anomaly breakdown          \n",
      "------------------  ------------------  ------------------  ------------------  ------------------------------------\n",
      "    wednesday             502350              326363              175987                                        \n",
      "                                                                                DoS GoldenEye               7917\n",
      "                                                                                DoS Hulk                  158680\n",
      "                                                                                DoS Slowhttptest            3707\n",
      "                                                                                DoS Slowloris               5683\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "day = DAY\n",
    "\n",
    "# Initializing the table.\n",
    "header = [\"DS\", \"TOTAL\", \"BENIGN\", \"ANOMALY\", \"Anomaly breakdown\"]\n",
    "rowh = \"{:^18}  \"*(len(header)-1) + \"{:^36}\"\n",
    "row  = \"{:^18}  \"*(len(header)-1) + \"{:<26}\" + \"{:>6}\"\n",
    "sep  = [\"-\"*18]*(len(header)-1) + [\"-\"*36]\n",
    "print(rowh.format(*header))\n",
    "print(rowh.format(*sep))\n",
    "\n",
    "csv = pd.read_csv(os.path.join(CSV_DIR,f\"{day}_{SCENARIO}.csv\"))\n",
    "\n",
    "TOTAL = len(csv)\n",
    "BENIGN = len(csv[csv[\"label\"] == \"BENIGN\"])\n",
    "ANOMALY = len(csv[(csv[\"label\"] != \"BENIGN\") & (csv[\"label\"] != \"NaN\")])\n",
    "\n",
    "print(row.format(DAY, TOTAL, BENIGN, ANOMALY, \"\", \"\"))\n",
    "for label in sorted(csv[\"label\"].unique().tolist()):\n",
    "    if label == \"BENIGN\":\n",
    "        continue\n",
    "    print(row.format(\"\",\"\",\"\",\"\", label, len(csv[csv[\"label\"] == label])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9dd98-1690-4582-89bb-4a53be2defc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
